\documentclass[11pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{What are the odds?}
\title{Manuale dell'ingegnere intrippato con la statistica}

\maketitle

\medskip
\section{Statistica descrittiva}
\subsection{Le grandezze che sintetizzano i dati}
\subsubsection{Media}
Dato un insieme $x_1,x_2,...,x_n$ di dati, si dice media campionaria la media aritmetica di questi valori.
\begin{displaymath}
\overline{x}:=\frac{1}{n}\sum_{i=1}^{n}x_i
\end{displaymath}
\subsubsection{Mediana}
Dato un insieme di dati di ampiezza $n$, lo si ordini dal minore al maggiore. La mediana è il valore che occupa la posizioone $\frac{n+1}{2}$ in caso di un insieme dispari, o la media tra $\frac{n}{2}$ e $\frac{n}{2}+1$ se pari.
\subsubsection{Moda}
La moda campionaria di un insieme di dati, se esiste, è l'unico valore che ha frequenza massima.
\subsubsection{Varianza e deviazione standard campionarie}
Dato un insieme di dati $x_1,x_2,...,x_n$, si dice varianza campionaria ($s^2$), la quantità
\begin{displaymath}
s^2:=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2
\end{displaymath}
Una comodità per il calcolo è che 
\begin{displaymath}
\sum_{i=1}^{n}(x_i-\overline{x})^2 = \sum_{i=1}^{n}x_i^2-n\overline{x}^2
\end{displaymath}
Si dice \textbf{deviazione standard campionaria} e si denota con $s$, la quantità
\begin{displaymath}
s:=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2}
\end{displaymath}
\begin{center}
    (la radice quadrata di $s^2$)
\end{center}
\subsubsection{Percentili campionari e box plot}
Sia k un numero intero $0\le k \le 100$. Dato un campione di dati, esiste sempre un dato che è contemporaneamente maggiore del $k$ percento dei dati, e minore del $100-k$ percento. Per trovare questo dato, dati $n$ e $p=\frac{k}{100}$:
\begin{enumerate}
    \item Disponiamo i dati in ordine crescente
    \item Calcoliamo $np$
    \item Il numero cercato è quello in posizione $np$, arrotondato per eccesso se non intero.
\end{enumerate}
Il 25-esimo percentile si dice \textit{primo quartile}, il 50-esimo \textit{secondo} (ed è pari alla mediana), il 75-esimo \textit{terzo}. Il box plot è un grafica con un quadrato sulla linea dei dati, con i lati sul primo e terzo quartile, e un segno sul secondo.
\subsection{Disuguaglianza di Chebyshev}
Siano $\overline{x}$ e $s$ media e deviazione standard campionarie di un insieme di dati. Nell'ipotesi che $s>0$, la disuguaglianza di Chebyshev afferma che per ogni reale $k\ge 1$, almeno una frazione $(1-1/k^2)$ dei dati cade nell'intervallo che va da $\overline{x}-ks$ a $\overline{x}+ks$.
Usando il \sout{pessimo} \textit{fantastico} linguaggio da statista: sia assegnato un insieme di dati $x_1,...,x_n$ con media campionaria $\overline{x}$ e deviazione standard campionaria $s>0$. Denotiamo con $S_k$ l'insieme degli indici corrispondenti a dati compresi tra $\overline{x}-ks$ e $\overline{x}+ks$. Sia $\#S_k$ il numero dei suddetti. Allora abbiamo che
\begin{displaymath}
\frac{\#S_k}{n}\ge 1 - \frac{n-1}{nk^2} >1-\frac{1}{k^2}
\end{displaymath}
\subsection{Insiemi di dati bivariati e coefficiente di correlazione campionaria}
A volte non abbiamo a che fare con dati singoli, ma con coppie di numeri, tra i quali sospettiamo l'esistenza di relazioni. Dati di questa forma prendono il nome di \textit{campione bivariato}. Uno strumento utile è il diagramma di dispersione. Una questione interessante è capire se vi sia correlazione tra i dati accoppiati. Parleremo di correlazione positiva quando abbiamo una proporzionalità diretta tra i due, di correlazione negativa quando abbiamo una proporzionalità inversa.
\subsubsection{Coefficiente di correlazione campionaria}
Dato un campione bivariato $(x_i,y_i)$, sono definite le medie $\overline{x}$ e $\overline{y}$. Possiamo senz'altro dire che se un valore $x_i$ è grande rispetto alla media, la differenza $x_i-\overline{x}$ sarà positiva, mentre se $x_i$ è piccolo, la differenza sarà negativa. Quindi, considerando il prodotto $(x_i-\overline{x})(y_i-\overline{y})$, sarà positivo per correlazioni positive, negativo per correlazioni negative. Se l'intero campione mostra quindi un'elevata correlazione, ci aspettiamo che la somma di tutti i prodotti $\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})$ darà una buona stima della correlazione. Normalizziamola dividendo per $(n-1)$ e per il prodotto delle deviazione standard campionarie, e otteniamo il \textbf{coefficiente di correlazione campionaria}
\begin{displaymath}
r:=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{(n-1)s_x s_y}
\end{displaymath}
con $s_x$ e $s_y$ deviazioni standard campionarie di $x$ e $y$.
\subsubsection{Proprietà del coefficiente di correlazione campionaria}
Sebbene parleremo meglio di questo bastardo nella sezione sulla regressione, elenchiamo qui alcune proprietà:
\begin{enumerate}
    \item $-1 \le r \le 1$
    \item Se per opportune costanti a e b, con $b>0$ sussiste la relazione lineare $y_i = a+b_x$, allora $r=1$.
    \item Se per opportune costanti a e b, con $b<0$ sussiste la relazione lineare $y_i = a+b_x$, allora $r=-1$.
    \item Se $r$ è il coefficiente di correlazione del campione $(x_i, y_i)$, $i=1,...,n$, allora lo è anche per il campione $(a+bx_i, c+dy_i)$, purché le costanti $a$ e $b$ abbiano lo stesso segno.
\end{enumerate}
\section{Elementi di probabilità}
\subsection{Spazio degli esiti ed eventi}
Si dice spazio degli esiti l'insieme di tutti gli esiti possibili di un esperimento. Se ad esempio l'esito dell'esperimento fosse il sesso di un neonato, lo spazio degli esiti sarebbe 
\begin{displaymath}
S=\{f,m\}
\end{displaymath}
I sottoinsiemi dello spazio degli esiti si dicono \textbf{eventi}, quindi un evento E è un insieme i cui elementi sono esiti possibili. Si dice $E^c$ l'opposto dell'evento, quindi $P(E^c) = 1 - P(E)$. Risulta ovvio che $1 = P(E^c) + P(E)$.
Se abbiamo due eventi qualsiasi, la loro unione $P(E\cup F) = P(E) + P(F) - P(E\cap F)$.
\subsection{Spazi di esiti equiprobabili}
Per tanti esperimenti è naturale assumere che ognuno degli esiti abbia la stessa probabilità di accadere. Abbiamo quindi che la probabilità che $E$ accada è pari a $P(E)=\frac{1}{N}$.
\subsubsection{Principio di enumerazione}
Consideriamo la realizzazione di due diversi esperimenti che possono avere rispettivamente $m$ ed $n$ esiti. Allora complessivamente avremo $mn$ risultati.
\subsection{Assiomi della probabilità}
Enunciamo ora tre fondamentali assiomi, così importanti da essere definiti i \textbf{tre assiomi della probabilità.}
\begin{enumerate}
    \item $0\le P(E) \le 1$, ossia la probabilità è sempre compresa tra 0 e 1
    \item $P(S)=1$, ossia la probabilità dello spazio degli esiti è 1. Questo significa che lo spazio degli esiti include sempre tutti gli esiti, quindi dovrà risultare in un 100\% di possibilità.
    \item $P\left(\bigcup_{i=1}^n E_i\right) = \sum_{i=1}^n P(E_i)$, ossia che unendo due eventi, le loro probabilità si sommano.
\end{enumerate}
\subsection{Coefficiente binomiale}
Vogliamo ora determinare il numero di diversi gruppi di $r$ oggetti che si possono formare scegliendoli da un insieme di $n$. Ad esempio, quanti gruppi di 3 lettere possono formarsi dal gruppo \{A,B,C,D,E\}. In generale, poiché il numero di modi diversi di scegliere $r$ oggetti su $n$ tenendo conto dell'ordine è dato da $n(n-1)...(n-r+1)$, e poiché ogni gruppo di lettere viene contato $r!$ volte (uno per permutazione), il numero di gruppi di $r$ elementi su n totali è dato da
\begin{displaymath}
    \frac{n(n-1)...(n-r+1)}{r!} = \frac{n!}{r!(n-r!)} = {n\choose r}
\end{displaymath}
\subsection{Probabilità condizionata}
Vogliamo ora calcolare la probabilità che un evento accada, appurato che ne è accaduto un altro. Ad esempio, lanciamo due dadi. L'evento E cercato è che il risultato sia 8. L'evento F già accaduto è che il primo dato risulta in un 3. Si dice probabilità condizionata di E dato F
\begin{displaymath}
    P(E|F) = \frac{P(E\cap F)}{P(F)}
\end{displaymath}
\subsection{Fattorizzazione di un evento e formula di Bayes}
Siano E ed F due eventi qualsiasi. È possibile esprimere E come
\begin{displaymath}
    P(E) = P(E\cap F) + P(E\cap F^c)
\end{displaymath}
Visto inoltre che i due sono eventi disgiunti, si ha che
\begin{gather*}
    P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\\
    P(E|F)P(F) + P(E|F^c)[1-P(F)]
\end{gather*}
Questa \textit{orribile} equazione, ci mostra che la probabilità dell'evento E si può ricavare come media pesata delle probabilità condizionali di E sapendo che: F si è verificato e non si è verificato. I pesi sono ovviamente le probabilità degli eventi a cui si condiziona.
L'equazione di cui sopra si può generalizzare in questo modo: prendiamo $F_n$ eventi mutualmente esclusivi, tali che 
\begin{displaymath}
    \bigcup_{i=1}^nF_i=S
\end{displaymath}
Risulta evidente che
\begin{displaymath}
    P(E)=\sum_{i=1}^n P(E\cap F_i)=\sum_{i=1}^n P(E|F_i)P(F_i)
\end{displaymath}
Da questa \textbf{formula di fattorizzazione} è possibile ricavare la \textbf{formula di Bayes}, che afferma che
\begin{displaymath}
    P(F_j|E)=\frac{P(F_j \cap E)}{P(E)}=\frac{\sum_i P(E|F_j)P(F_j)}{\sum_i P(E|F_i)P(F_i)}
\end{displaymath}
\subsection{Eventi indipendenti}
Due eventi si dicono indipendenti quando il risultato di uno non influenza l'altro. In altre parole, significa che avendo due eventi E ed F, se so che F è accaduto, la probabilità che accada E non cambia.
\begin{displaymath}
    P(E\cap F) = P(E) P(F)
\end{displaymath}
\section{Variabili aleatorie e valore atteso}
Quando realizziamo un esperimento casuale, non sempre siamo interessati a tutti i risultati del suddetto. Se ad esempio lanciassimo due dadi, potrebbe interessarci la sola somma e non i singoli risultati. 
Queste quantità di interesse sono dette \textbf{variabili aleatorie}. Siccome il valore di questa variabile è dato dal risultato dell'esperimento, possiamo assegnare delle probabilità a queste. Queste variabili aleatorie hanno una \textit{funzione indicatrice} definita, ad esempio, così:
\begin{displaymath}
    I:=
    \begin{cases}
        1 \mbox{ se } X = 1 \mbox{ o } 2\\
        0 \mbox{ se } X = 0
    \end{cases}
\end{displaymath}
Variabili aleatorie con un numero finito o numerabile di valori possibili sono dette \textbf{discrete}. Esistono anche variabili aleatorie \textbf{continue}.
\subsubsection{Funzione di ripartizione}
La funzione di ripartizione F di una variabile aleatoria X, è definita, per ogni numero reale $x$, tramite
\begin{displaymath}
    F(x) := P(X\le x)
\end{displaymath}
Quindi $F(x)$ esprime la probabilità che la variabile aleatoria X assuma un valore \textit{minore o uguale} a $x$. Tutte le questioni di probabilità che si possano sollevare su una variabile aleatoria, ammettono una risposta in termini della sua funzione di ripartizione.
\subsection{Variabili aleatorie discrete e continue}
Se X è una variabile aleatoria discreta, la sua funzione di massa di probabilità, o funzione di massa, si definisce nel modo seguente:
\begin{displaymath}
    p(a) :=P(X=a)
\end{displaymath}
La funzione $p(a)$ è non nulla su un insieme al più numerabile di valori. Infatti, se $x_1,x_2,...,x_n$ sono i possibili valori di X, allora
\begin{gather*}
    p(x_i) > 0 \hspace{10px}i=1,2,...\\
    p(x) = 0 \hspace{10px}\mbox{tutti gli altri valori di x}
\end{gather*}
Siccome X deve assumere i suddetti valori, necessariamente deve essere vero che
\begin{displaymath}
    \sum_{i=1}^\infty p(x_i) = 1
\end{displaymath}
Una variabile aleatoria che possa assumere un'infinità non numerabile di valori, non potrà essere discreta. Si dirà \textbf{continua} se esiste una funzione non negativa $f$, definita su tutto $\mathbb{R}$, avente la proprietà che per ogni insieme B di numeri reali,
\begin{displaymath}
    P(X\in B) = \int_B f(x) dx
\end{displaymath}
Questa funzione è detta \textbf{funzione di densità di probabilità}. L'equazione dice che la probabilità che una variabile aleatoria continua X appartenga a un insieme B si può trovare integrando la sua densità su tale insieme. Pare ovvio che
\begin{displaymath}
    1=P(X\in \mathbb{R}) = \int_{-\infty}^{\infty} f(x)dx
\end{displaymath}
Tutte le probabilità che riguardano una variabile aleatoria continua possono essere espresse in funzione della sua densità di probabilità:
\begin{displaymath}
    P(a\le X \le b) = \int_a^b f(x)dx
\end{displaymath}
Se poniamo $b=a$, notiamo che la probabilità che una variabile aleatoria continua assuma un valore particolare $a$ è nulla:
\begin{displaymath}
    P(X=a) = \int_a^a f(x)dx = 0
\end{displaymath}
Leghiamo la funzione di ripartizione F alla densità $f$ così:
\begin{displaymath}
    F(a) := P(X \in (-\infty,a]) = \int_{-\infty}^a f(x)dx
\end{displaymath}
Derivando entrambi otteniamo la relazione fondamentale:
\begin{displaymath}
    \frac{d}{da}F(a) = f(a)
\end{displaymath}
La densità è quindi la derivata della funzione di ripartizione. 
Notiamo che quando conosciamo la funzione di massa di probabilità di una variabile aleatoria discreta, o la funzione di densità di probabilità di una continua, abbiamo abbastanza informazioni per poter calcolare le probabilità di ogni evento che dipenda dalla sola variabile aleatoria. 
\subsection{Coppie e vettori di variabili aleatorie}
Ci sono situazioni in cui abbiamo necessità di studiare le \textbf{relazioni} tra variabili aleatorie multiple. Per specificare la relazione tra due variabili aleatorie X e Y, il primo passo è estendere il concetto di funzione di ripartizione.
Siano quindi X e Y due variabili aleatorie che riguardano lo stesso esperimento casuale. Si dice \textit{funzione di ripartizione congiunta} di X e Y la funzione di due variabili seguente:
\begin{displaymath}
    F(x,y) := P(X\le x, Y \le y)
\end{displaymath}
dove la virgola denota l'intersezione tra gli eventi.
La conoscenza di questa funzione permette, almeno in teoria, di calcolare le probabilità di tutti gli eventi che dipendono, singolarmente o congiuntamente, da X e Y. 
\subsubsection{Distribuzione congiunta per variabili aleatorie discrete}
Se sappiamo che un vettore aleatorio è di tipo discreto, possiamo definire e utilizzare la funzione di massa di probabilità.
Se X e Y sono variabili aleatorie discrete che assumono i valori $x_1, x_2,...$ e $y_1,y_2,...$, la funzione
\begin{displaymath}
    p(x_i, y_j) := P(X=x_i, Y=y_j),\hspace{10px}i=1,2,...\hspace{5px}j=1,2,...
\end{displaymath}
è la loro funzione di massa di probabilità congiunta.
Le funzioni di massa individuali si possono ricavare da questa, notando che, siccome Y deve assumere uno dei valori $y_j$, l'evento $\{X=x_i\}$, può essere visto come l'unione al variare di $j$ degli eventi $\{X=x_i, Y=y_j\}$, che sono mutuamente esclusivi. Da qui:
\begin{displaymath}
    p_X(x_i) := P(X=x_i) = \sum_j p(x_i, y_j)
\end{displaymath}
Anche se le individuali possono essere ricavate dalla congiunta, la congiunta non può essere ricavata dalle condizionali. 
\subsubsection{Distribuzione congiunta per variabili aleatorie continue}
Due variabili aleatorie X e Y sono congiuntamente continue se esiste una funzione non negativa $f(x,y)$ tale che, per ogni sottoinsieme C del piano cartesiano,
\begin{displaymath}
    P((X,Y)\in C) = \int\int_{(x,y)\in C} f(x,y)dxdy
\end{displaymath}
questa è detta \textbf{densità congiunta} delle variabili aleatorie X e Y.
Otteniamo inoltre che
\begin{displaymath}
    P(X \in A, Y \in B) = \int_B\int_A f(x,y)dxdy
\end{displaymath} 
E, in conclusione,
\begin{displaymath}
    P(X \in A) = \int_A f_X(x) dx
\end{displaymath}
Per ricavare le individuali, otteniamo
\begin{gather*}
    f_X(x) =\int_{-\infty}^\infty f(x,y) dy\\
    f_Y(y) =\int_{-\infty}^\infty f(x,y) dx
\end{gather*}
\subsubsection{Variabili aleatorie indipendenti}
Due variabili aleatorie sono indipendenti se tutti gli eventi relativi alla prima sono indipendenti da tutti quelli relativi alla seconda. 
La definizione è che se, per ogni coppia di insiemi di numeri reali A e B è soddisfatta
\begin{displaymath}
    P(X \in A, Y \in B) = P(X\in A)P(Y \in B)
\end{displaymath}
le due V.A. sono indipendenti. 
Se le V.A. sono discrete, l'equazione equivale a dire che la funzione di massa congiunta è il prodotto delle marginali:
\begin{displaymath}
    p(x,y) = p_X(x) p_Y(y)
\end{displaymath}
Possiamo generalizzare le osservazioni suddette anche per vettori di variabili aleatorie. \textit{Lo faremo? Non credo proprio.}
\subsubsection{Distribuzioni condizionali}
Le relazioni esistenti tra due variabili aleatorie possono essere chiarite dallo studio della distribuzione condizionale di una delle due, dato il valore dell'altra. Si ricorda che che presi comunque due eventi E e F con $P(F)>0$, la probabilità di E condizionata a F è data da
\begin{displaymath}
    P(E|F):=\frac{P(E\cap F)}{P(F)}
\end{displaymath}
è naturale applicare questo schema anche alle variabili aleatorie discrete.
Siano X e Y due variabili aleatorie discrete con funzione di massa congiunta $p(\cdot , \cdot)$, diciamo funzione di massa di probabilità condizionata di X dato Y e si indica con $p_{X|Y}(\cdot | \cdot)$, la funzione di due variabili così definita:
\begin{gather*}
    p_{X|Y}(x|y) := P(X=x|Y=y)\\ 
    \frac{p(x,y)}{p_Y(y)},\hspace{10px}\forall x \forall y \mbox{ con }p_Y(y)>0
\end{gather*}
Se $y$ non è un valore possibile di Y, ovvero se $P(Y=y)=0$, la quantità $p_{X|Y}(x|y)$ non è definita. 
\subsection{Valore atteso}
Uno dei concetti più importanti di tutta la teoria della probabilità \textit{(ziocane)} è quello di valore atteso. Esso è definito come il numero
\begin{displaymath}
    E[X]:=\sum_i x_i P(X=x_i)
\end{displaymath}
In altri termini, si tratta della media pesata dei valori possibili di X, usando come pesi le probabilità che vengano assunti. È ovvio che nel caso di V.A. continue, il giochino non funziona. Definiamo quindi il valore atteso di una V.A. continua con funzione di densità $f$, come
\begin{displaymath}
    E[X]:=\int_{-\infty}^\infty xf(x)dx
\end{displaymath}
\subsection{Proprietà del valore atteso}
Consideriamo una V.A. di cui conosciamo la distribuzione. \textit{What if, }se anziché calcolare il valore atteso di X, volessimo calcolare quello di una funzione $g(X)$? Notiamo che $g(X)$ è comunque una variabile aleatoria. Ricaviamo quindi la sua distribuzione, e ne calcoliamo il valore atteso. Ponendo le cose in maniera rigorosa:
\begin{gather*}
E[g(X)] = \sum_x g(x)p(x)\mbox{ per V.A. discrete}\\ 
E[g(X)] =\int_{-\infty}^\infty g(x)f(x)dx\mbox{ per V.A. continue}
\end{gather*}
Per ogni coppia di costanti reali $a$ e $b$, abbiamo anche che
\begin{displaymath}
    E[aX+b] = aE[X]+b\hspace{10px}\mbox{e quindi}\hspace{10px}E[aX]=aE[X]
\end{displaymath}
\subsubsection{Valore atteso della somma di variabili aleatorie}
Con \textit{complessi calcoli tendenzialmente inutili}, otteniamo che 
\begin{displaymath}
    E[X+Y] = E[X] + E[Y]
\end{displaymath}
Tale risultato vale sia nel caso discreto, che in quello continuo.
\subsection{Varianza}
A volte, conoscere la media di una distribuzione non basta. \textit{Se inserissimo l'autore di questi riassunti con la testa in un freezer e i piedi in un forno, la temperatura media sarebbe abbastanza ok, l'autore no.} Per questo, è utile conoscere quanto i valori si allontanano dalla media. Questo è proprio il compito della \textbf{varianza}. Sia X una variabile aleatoria con media $\mu$, la varianza di x, che denotiamo con $Var(X)$ è la quantità
\begin{displaymath}
    Var(X):=E[(X-\mu)^2]
\end{displaymath}
o, in alternativa (\textit{questa è molto più comoda})
\begin{displaymath}
    Var(X) = E[X^2]-E[X]^2
\end{displaymath}
\subsection{La covarianza e la varianza della somma di V.A.}
Come sappiamo, la media della somma di V.A. coincide con la somma delle loro medie. Per la varianza, in generale, questo non è vero. \textbf{In un caso sì: quando le V.A. sono indipendenti}. Prima di tutto, però, definiamo il concetto di Covarianza: date due V.A. X e Y di media $\mu_X$ e $\mu_Y$, essa vale
\begin{displaymath}
    Cov(X,Y) := E[(X-\mu_X)(Y-\mu_Y)]
\end{displaymath}
o, in alternativa
\begin{displaymath}
    Cov(X,Y) := E[XY] - E[X]E[Y]
\end{displaymath}
Derivano anche alcune semplici proprietà
\begin{gather*}
    Cov(X,Y)=Cov(Y,X)\\ 
    Cov(X,X)=Var(X)\\ 
    Cov(aX,Y)=aCov(X,Y)=Cov(X,aY)
\end{gather*}
E se avessimo 3 V.A.?
\begin{displaymath}
    Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)
\end{displaymath}
Inoltre, generalizzando i concetti, se avessimo n V.A. $X_1,...,X_n$ e n $Y_1,...,Y_n$
\begin{displaymath}
    Cov\left(\sum_{i=1}^nX_i,\sum_{j=1}^mY_j\right) = \sum_{i=1}^n\sum_{j=1}^m Cov(X_i,Y_j)
\end{displaymath}
\subsubsection{Variabili aleatorie indipendenti}
Se abbiamo due V.A. X e Y indipendenti, sappiamo che
\begin{displaymath}
    E[XY] = E[X]E[Y]
\end{displaymath}
Questo implica inoltre che
\begin{displaymath}
    Cov(X,Y) = 0
\end{displaymath}
e quindi, se abbiamo n V.A., la varianza della somma è la somma delle varianze.
\begin{displaymath}
    Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i)
\end{displaymath}
\section{La funzione generatrice dei momenti}
La \textit{funzione generatrice dei momenti}, o più semplicemente, la funzione generatrice $\phi$ di una V.A. X, è definita, per tutti i $t$ reali per i quali il valore atteso di $e^{tX}$ ha senso, dall'espressione
\begin{displaymath}
    \phi(t):=E[e^{tX}]=
    \begin{cases}
        \sum_x e^{tx}p(x)\hspace{10px}\mbox{se X è discreta}\\ 
        \int_{-\infty}^\infty e^{tx} f(x)dx\hspace{10px}\mbox{se X è continua}
    \end{cases}
\end{displaymath} 
Il nome deriva dal fatto che tutti i momenti di cui è dotata X possono essere ottenuti derivando più volte nell'origine la funzione $\phi(t)$. Ad esempio,
\begin{displaymath}
    \phi'(t)=\frac{d}{dt}E[e^{tX}]=E[Xe^{tX}]
\end{displaymath}
Quindi, $\phi'(0) = E[X]$, e, più in generale, $\phi^n(0) = E[X^n]$.
Se X e Y sono variabili indipendenti con funzioni generatrici $\phi_X$ e $\phi_Y$, e se $\phi_{X+Y}$ è la funzione generatrice dei momenti di $X+Y$, allora
\begin{displaymath}
    \phi_{X+Y}(t) = \phi_X(t)\phi_Y(t)
\end{displaymath}
Un'osservazione interessante sulla generatrice dei momenti, è che essa \textit{determina la distribuzione}, ossia se due V.A. hanno identica generatrice, hanno identica legge(quindi funzione di ripartizione e funzione di massa).
\section{La legge debole dei grandi numeri}
Per introdurre la suddetta, prima enunciamo la \textbf{disuguaglianza di Markov}: se X è una variabile aleatoria che non è mai negativa, allora per ogni $a>0$
\begin{displaymath}
    P(X\ge a) \le \frac{E[X]}{a}
\end{displaymath}
Come corollario, ricaviamo la disuguaglianza di Chebyshev: data una V.A. X con media $\mu$ e varianza $\sigma^2$, allora per ogni $r>0$
\begin{displaymath}
    P(|X-\mu|\ge r) \le \frac{\sigma^2}{r^2}
\end{displaymath}
Otteniamo infine la \textbf{legge debole dei grandi numeri}. \textit{No, non quella con cui giustificate il vostro provarci con ogni essere vivente femminile.} Sia $X_1,X_2,...$ una successione di variabili aleatorie i.i.d\textit{(indipendenti, identicamente distribuite)}, tutte con media $\mu$. Allora, per ogni $\epsilon >0$:
\begin{displaymath}
    P\left(\left|\frac{X_1+...+X_n}{n} - \mu\right|>\epsilon \right)\rightarrow0\hspace{10px}\mbox{quando }n\rightarrow\infty
\end{displaymath}
\textit{Ora, vi chiederete: cosa me ne faccio? Posso spiegarla alle tipe in discoteca?\textbf{ No.}} Un'applicazione interessante è la seguente: supponiamo di ripetere in successione molte copie indipendenti di un esperimento, in ciascuna delle quali può verificarsi un certo evento E:
\begin{displaymath}
    X_i:=
    \begin{cases}
        1\mbox{ se E si realizza nell'esperimento i-esimo}\\ 
        0\mbox{ se E non si realizza}
    \end{cases}
\end{displaymath}
la sommatoria $X_1+...+X_n$ rappresenta il numero di prove - tra le prime n - in cui si è verificato l'evento E. Poiché 
\begin{displaymath}
    E[X_i]=P(X_i=1)=P(E)
\end{displaymath}
si deduce che la frazione delle \textit{n} prove nelle quali si realizza E, tende alla probabilità $P(E)$.
\section{Modelli di variabili aleatorie}
\textit{Siamo giunti a una sezione tanto interessante, quanto fastidiosa: quella in cui dovete ricordare uno sh*t ton di roba}.
\subsection{Variabili aleatorie di Bernoulli e binomiali}
Supponiamo di fare un esperimento che ha solo due esiti, \textit{successo e fallimento}. Sappiamo che
\begin{gather*}
    P(X=0)=1-p\\ 
    P(X=1)=p
\end{gather*}
Una V.A. con funzione di massa di probabilità come questa, è detta \textbf{Bernoulliana}. Il suo valore atteso $E[X] = p$.
Supponiamo ora di realizzare n esperimenti, ciascuno dei quali è descritto da una Bernoulliana. Se X denota il numero totale di successi, X si dice V.A. binomiale di parametri $(n,p)$. La funzione di massa di probabilità è data da:
\begin{displaymath}
    P(X=i)={n \choose i} p^i(1-p)^{n-i}
\end{displaymath}
con il solito coefficiente binomiale
\begin{displaymath}
    {n \choose i} := \frac{n!}{i!(n-i)!}
\end{displaymath}
Si noti che la somma delle probabilità di tutti i valori possibili è ovviamente 1.
\begin{displaymath}
    \sum_iP(X=i)=[p+(1-p)]^n=1
\end{displaymath}
Osserviamo che se $X_1$ e $X_2$ sono binomiali di parametri $(n_1,p)$ e $(n_2,p)$ e sono indipendenti, allora la somma $X_1+X_2$ è binomiale di parametri $(n_1+n_2,p)$.
Sappiamo inoltre che
\begin{displaymath}
    E[X]=np\hspace{20px}Var(X)=np(1-p)
\end{displaymath}
\subsubsection{Calcolo esplicito della distribuzione binomiale}
Supponiamo che X sia binomiale di parametri $(n,p)$. Per poter calcolare operativamente la funzione di ripartizione:
\begin{displaymath}
    P(X\le i) = \sum_{k=0}^i {n \choose i} p^k (1-p)^{n-k}
\end{displaymath}
o la funzione di massa:
\begin{displaymath}
    P(X=i) = {n \choose i} p^i(1-p)^{n-k}
\end{displaymath}
è molto utile sapere che 
\begin{displaymath}
    P(X=k+1)=\frac{p}{1-p}\frac{n-k}{k+1}P(X=k)
\end{displaymath}
\subsection{Variabili aleatorie di Poisson}
Le variabili aleatorie di Poisson assumono solo valori interi non negativi.
In altri termini, definiamo \textit{una variabile aleatoria X che assuma i valori 0,1,2...} \textbf{poissoniana} di parametro $\lambda$, $\lambda>0$, se la sua funzione di massa di probabilità è data da
\begin{displaymath}
    P(X=i) = \frac{\lambda^i}{i!}e^{-\lambda}
\end{displaymath}
Calcoliamo la generatrice dei momenti $E[e^{tX}] = e^{-\lambda}e^{\lambda e^t}$, per poi derivarla e calcolarla in $X=0$.
Troviamo quindi che 
\begin{gather*}
    E[X] = \phi'(0) = \lambda\\
    Var(X) = \phi''(0)-E[X]^2 = \lambda
\end{gather*}
Una caratteristica interessante della poissoniana è che può essere utilizzata come approssimazione di una binomiale di parametri $(n,p)$, quando $n$ è molto grande e $p$ molto piccolo. 
Ponendo $\lambda=np$:
\begin{displaymath}
    P(X=i) \approx \frac{\lambda^i}{i!}e^{-\lambda}
\end{displaymath}
In altri termini, il totale dei successi di un numero n molto elevato di ripetizioni dell'esperimento, è una variabile aleatorie di con distribuzione approsimativamente di Poisson, con media $\lambda=np$.
\subsubsection{Calcolo esplicito della distribuzione di Poisson}
Se X è una variabile aleatoria di Poisson di media $\lambda$, allora
\begin{displaymath}
    \frac{P(X=i+1)}{P(X=i)}=\frac{\lambda^{i+1}e^{-\lambda}}{(i+1)!}\frac{i!}{\lambda^i e^{-\lambda}}=\frac{\lambda}{i+1}
\end{displaymath}
\textit{A cosa serve sto pippozzo? }Possiamo usarla per calcolare, a partire da $P(X=0) = e^{-\lambda}$, tutti gli altri. Ad esempio:
\begin{gather*}
    P(X=1) = \lambda P(X=0)\\
    P(X=2) = \frac{\lambda}{2} P(X=1)
\end{gather*}
\subsection{Variabili aleatorie ipergeometriche}
Una scatola contiene N batterie accettabili, ed M difettose. Si estraggono senza rimessa $n$ batterie, dando pari probabilità a ciascuno degli ${{N+M} \choose n}$ sottoinsiemi possibili. Se denotiamo con X il numero di batterie accettabili estratte,
\begin{displaymath}
    P(X=i)=\frac{{N \choose i} {N \choose {n-i}}}{{{N+M} \choose n}}\hspace{10px}i=0,1,...,n
\end{displaymath}
Una V.A. con massa di probabilità data da questa equazione, è detta \textbf{variabile aleatoria ipergeometrica} di parametri N, M e $n$.
\subsection{Variabili aleatorie uniformi}
Una variabile aleatoria continua si dice uniforme sull'intervallo $[\alpha,\beta]$, se ha funzione di densità data da:
\begin{displaymath}
    f(x)=
    \begin{cases}
        \frac{1}{\beta-\alpha} \mbox{ se }\alpha\le x \le \beta\\
        0\hspace{10px}\mbox{altrimenti} 
    \end{cases}
\end{displaymath}
Si noti che soddisfa le condizioni per essere una densità di probabilità, in quanto
\begin{displaymath}
    \int_{-\infty}^\infty f(x)dx =\frac{1}{\beta-\alpha}\int_\alpha^\beta dx=1
\end{displaymath}
Per poter assumere la distribuzione uniforme, nella pratica, occorre che la V.A. abbia come valori possibili i punti di un intervallo limitato $[\alpha,\beta]$, inoltre si deve poter supporre che essa abbia le stesse probabilità di cadere vicino a un qualunque punto dell'intervallo.
La probabilità che una V.A. X, uniforme su $[\alpha,\beta]$ è pari al rapporto tra le lunghezze dei due intervalli,. Infatti, se $[a,b]$ è contenuto in $[\alpha,\beta]$
\begin{displaymath}
    P(a<X<b)=\frac{1}{\beta-\alpha}\int_a^b dx = \frac{b-a}{\beta-\alpha}
\end{displaymath}
\subsection{Variabili aleatorie normali}
Questa è la sezione più applicabile alla vita reale. \textit{Quella che, a forza di studiare statistica, avete dimenticato.} 
Una variabile aleatoria X si dice \textbf{normale} oppure \textbf{gaussiana} di parametri $\mu,\sigma^2$ e si scrive $X \sim N(\mu,\sigma^2)$ se X ha funzione di densità data da
\begin{displaymath}
    f(x)= \frac{1}{\sqrt{2\pi}\sigma} exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} \hspace{10px}\forall x \in \mathbb{R} 
\end{displaymath}
La densità normale è una curva a campana simmetrica rispetto all'asse $x=\mu$, dove ha il massimo pari a $(\sigma\sqrt{2\pi})^{-1} \approx  0.399/\sigma$.
Come sempre, cerchiamo valore atteso e varianza, calcolando la generatrice $\phi = E[e^{tX}]$, derivandola e calcolandola in $X=0$. Otteniamo 
\begin{gather*}
    E[X]=\phi'(0)=\mu\\ 
    Var(X) = E[X^2]- E[X]^2 = \sigma^2
\end{gather*}
Un risultato importante è che se X è una gaussiana e Y una trasformazione lineare di X, allora Y è a sua volta una gaussiana. In altri termini:
sia $X\sim N(\mu, \sigma^2)$ e sia $Y=\alpha X + \beta$, dove $\alpha$ e $\beta$ sono due costanti reali, con $\alpha\neq 0$. Allora Y è una normale con media $\alpha\mu + \beta$ e varianza $\alpha^2\sigma^2$.
Un corollario della suddetta permette di ottenere il fantomatico processo di normalizzazione: se $X\sim N(\mu,\sigma^2)$, allora
\begin{displaymath}
    Z:=\frac{X-\mu}{\sigma}
\end{displaymath}
è una V.A. normale con media 0 e varianza 1. Questa roba è così importante che ha un nome, \textbf{normale standard}, e un simbolo:
\begin{displaymath}
    \Phi(x) := \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{\frac{-y^2}{2}}dy\hspace{10px}\forall x \in \mathbb{R}
\end{displaymath}
Il fatto che $Z:=(X-\mu)/\sigma$ abbia distribuzione normale standard quando X è gaussiana di media $\mu$ e varianza $\sigma^2$, ci permette di esprimere le probabilità relative di X in termini di probabilità su Z. Ad esempio per trovare $P(X<b)$, notiamo che $X<b$ s.s.se 
\begin{displaymath}
    \frac{X-\mu}{\sigma} < \frac{b-\mu}{\sigma}
\end{displaymath}
così che
\begin{displaymath}
    P(X<b) = \Phi\left(\frac{b-\mu}{\sigma}\right)
\end{displaymath}
Analogamente, per ogni $a<b$, si ha che 
\begin{displaymath}
    P(a<X<b) := \Phi\left(\frac{b-\mu}{\sigma}\right)-\Phi\left(\frac{a-\mu}{\sigma}\right)
\end{displaymath}
\subsection{Variabili aleatorie esponenziali}
Una V.A. continua, la cui funzione di densità è data da 
\begin{displaymath}
    f(x)=\begin{cases}
        \lambda e^{-\lambda x} \hspace{10px}\mbox{se }x\ge 0\\ 
        0\hspace{10px}\mbox{se }x<0
    \end{cases}
\end{displaymath}
per un opportuno valore della costante $\lambda>0$, si dice \textbf{esponenziale} con parametro (\textit{o intensità}) $\lambda$.
La funzione di ripartizione di una tale variabile è data da 
\begin{displaymath}
    F(x) = P(X\le x) = 1-e^{-\lambda x}\hspace{10px} x\ge 0
\end{displaymath}
Nella pratica, la distribuzione esponenziale può rappresentare il tempo di attesa prima che si verifichi un certo evento casuale. Calcoliamo la generatrice dei momenti $E[e^{tX}] = \frac{\lambda}{\lambda-t}$ con $t<\lambda$. Derivando e calcolando in 0:
\begin{gather*}
    E[X]=\phi'(0)=\frac{1}{\lambda}\\ 
    Var(X) = E[X^2] - E[X]^2 = \frac{1}{\lambda^2}
\end{gather*}
La proprietà fondamentale dell'esponenziale è la sua \textbf{assenza di memoria}. \textit{Che cazzo vuol dire? Nemmeno io ho memoria ma non ho libri da 40 euro che parlano di me.} Immaginiamo che X rappresenti il tempo che passa prima che un oggeto si rompa, dato t il tempo per cui ha già funzionato. Chiaramente, la probabilità che l'oggetto duri ancora per un tempo $s$ è $P(X>s+t | X>t)$. Otterremo che la condizione $X>t$ non varia la probabilità, e non ci dà quindi "informazioni utili".
Un'altra informazione utile, è che se abbiamo n V.A. esponenziali $X_1,...,X_n$ indipendenti, di parametri $\lambda_1,...,\lambda_n$, allora la V.A. $Y:=min(X_1,...,X_n)$ è un'esponenziale di parametro $\lambda_Y=\sum_{i=1}^n \lambda_i$.
\subsubsection{Il processo di Poisson}
Consideriamo una serie di eventi istantanei che avvengono a intervalli di tempo casuali, e sia $N(t)$ il numero di quanti se ne sono verificati nell'intervallo $[0,t]$. $N(t)$ si dice \textit{processo di Poisson} di intensità $\lambda$, $\lambda>0$, se
\begin{itemize}
    \item $N(0) = 0$: \textit{questa stabilisce che si iniziano a contare gli eventi al tempo 0.}
    \item Il numero degli eventi che hanno luogo in intervalli di tempo disgiunti sono indipendenti
    \item La distribuzione del numero di eventi che si verifica in un dato intervallo di tempo dipende solo dalla lunghezza dell'intervallo, e non dalla sua posizione.
    \item $\lim_{h\to0}\frac{P(N(h)=1)}{h} = \lambda$
    \item $\lim_{h\to0}\frac{P(N(h)\ge2)}{h} = 0$
\end{itemize}
Le ultime due affermano che se si considera un intervallo di tempo di lunghezza $h$, vi è approssimativamente possibilità $\lambda h$ che vi occorrà un evento solo, e circa una probabilità nulla che se ne verifichino due o più.
Se $N(t)$ è un processo di Poisson di intensità $\lambda$, allora 
\begin{displaymath}
    P(N(t)=k) = \frac{(\lambda t)^k}{k!} e^{-\lambda t}
\end{displaymath}
Ovvero, il numero di eventi in $[0,t]$ ha distribuzione di Poisson di media $\lambda t$. Inoltre, i tempi che separano gli eventi di un processo di Poisson, sono una successione di V.A. esponenziali di intensità $\lambda$.
\subsection{Variabili aleatorie di tipo gamma}
Una variabile aleatoria continua si dice avere distribuzione di tipo gamma di parametri $(\alpha,\lambda)$, con $\alpha>0, \lambda>0$ se la sua funzione di densità di probabilità è data da 
\begin{displaymath}
    f(x) =
    \begin{cases}
        \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x} \hspace{10px}\mbox{se } x>0\\ 
        0  \hspace{10px}\mbox{se } x\le0
    \end{cases}
\end{displaymath}
dove $\Gamma$ denota la funzione gamma di Eulero, che è definita in modo da normalizzare l'integrale di $f$.
Per proprietà che ci asterremo dall'enunciare, possiamo calcolare la gamma sugli interi così:
\begin{displaymath}
    \Gamma(n) = (n-1)!
\end{displaymath}
Si noti che per $\alpha=1$ la distribuzione gamma coincide con l'esponenziale. Troviamo ora la generatrice dei momenti, la deriviamo e calcoliamo in 0 per ottenere:
\begin{gather*}
    E[X] = \phi'(0) = \frac{\alpha}{\lambda}\\
    Var(X) = E[X^2]-E[X]^2 = \frac{\alpha}{\lambda^2}
\end{gather*}
Se abbiamo più V.A. indipendenti di tipo gamma $X_i$, allora $\sum_i^n X_i$ è una gamma di parametri $(\sum_i^n \alpha_i,\lambda)$. Di conseguenza, se abbiamo una somma di esponenziali, avremo una gamma di parametri $(n, \lambda)$.
Se $Z_1, Z_2, ..., Z_n$ sono V.A. normali standard e indipendenti, allora la somma dei loro quadrati è una V.A. che prende il nome di \textbf{chi-quadro a n gradi di libertà}, e si indica con
\begin{displaymath}
    X \sim \chi_n^2
\end{displaymath}  
Essa è riproducibile, nel senso che se $X_1$ e $X_2$ sono due chi-quadro indipendenti, $X_1+X_2$ è una chi-quadro con $n_1+n_2$ gradi di libertà. Per dimostrare questo fatto non è necessario ricorrere alle funzioni generatrici, perché dalla definizione è evidente che $X_1+X_2$ è la somma dei quadrati di $n_1+n_2$ normali srandard indipendenti. 
Definiamo la quantità $\chi_{\alpha,n}^2$, con $\alpha$ reale tra 0 e 1:
\begin{displaymath}
    P(X\ge \chi_{\alpha,n}^2) = \alpha
\end{displaymath}
\subsubsection{La relazione tra chi-quadro e gamma}
Una chi-quadro ad n gradi di libertà, corrisponde a una gamma di parametri $(n/2, 1/2)$. La densità di probabilità è perciò data da:
\begin{displaymath}
    f(x) = \frac{x^{n/2-1}e^{-x/2}}{2^{n/2}\Gamma(n/2)}
\end{displaymath}
\subsection{Le distribuzioni T}
Se $Z$ e $C_n$ sono V.A. indipendenti, la prima normale std e la seconda chi-quadro con $n$ gradi di libertà, allora la V.A. $T_n$, definita come 
\begin{displaymath}
    T_n := \frac{Z}{\sqrt{C_n/n}}
\end{displaymath}
si dice avere distribuzione t con n gradi di libertà, denotata con
\begin{displaymath}
    T_n \sim t_n
\end{displaymath}
La densità delle distribuzioni $t$ è simmetrica rispetto all'asse di ascissa 0. È possibile dimostrare che al crescere di $n$, la densità di $t_n$ converge a quella della normale standard. 
Come sempre, calcoliamo valore atteso e varianza:
\begin{gather*}
    E[T_n] = 0\\ 
    Var(T_n) = \frac{n}{n-2}
\end{gather*}
Si noti che, al crescere di $n$, la varianza decresce, convergendo a 1, cioè alla varianza della n.s. In analogia con quanto fatto per le chi-quadro, definiamo $t_{\alpha,n}$
\begin{displaymath}
    P(T_n\ge t_{\alpha,n}) = \alpha
\end{displaymath}
Dalla simmetria rispetto allo zero otteniamo che
\begin{displaymath}
    -t_{\alpha,n} = t_{1-\alpha,n}
\end{displaymath}
\subsection{Le distribuzioni F}
Se $C_n$ e $C:m$ sono V.A. indipendenti, di tipo chi-quadro con $n$ e $m$ gradi di libertà, allora la V.A. $F_{n,m}$ definita da:
\begin{displaymath}
    F_{n,m} :=\frac{C_n/n}{C_m/m}
\end{displaymath}
si dice avere distribuzione F con $n$ e $m$ gradi di libertà. Come sempre, definiamo
\begin{displaymath}
    P(F_{n,m}>P_{\alpha,n,m}) = \alpha
\end{displaymath}
Notiamo, con calcoli, che
\begin{displaymath}
    \frac{1}{F_{\alpha,n,m}}=F_{1-\alpha,n,m}
\end{displaymath}
\subsection{Distribuzione logistica}
Una V.A. continua si dice avere distribuzione \textbf{logistica} di parametri $(\mu, \nu)$, con $\nu>0$, se la sua funzione di ripartizione è data da
\begin{displaymath}
    F(x) = \frac{e^{(x-\mu)/\nu}}{1+e^{(x-\mu)/\nu}}\hspace{10px}\forall x \in \mathbb{R}
\end{displaymath}
\textit{Bella eh?} Cerchiamo il valore atteso, e scopriamo che $E[X] = \mu$. $\nu$ è invece detta \textit{dispersione}. Una V.A. logistica con media 0 e dispersione 1 è detta logistica standard.
\section{La distribuzione delle statistiche campionarie}
Se dovessimo studiare le caratteristiche di una popolazione molto grande, risulta ovvia l'impossibilità di analizzarne ogni singolo individuo. Si procede quindi solitamente a prenderne un campione casuale, e farvi inferenza.
Un insieme $X_1,X_2,...,X_n$ di V.A. indipendenti, tutte con la stessa distribuzione F, si dice campione o campione aleatorio della distribuzione F.
\subsection{La media campionaria}
Consideriamo una popolazione di elementi, a ognuno dei quali è associata una grandezza numerica. Denotiamo con $\mu$ e $\sigma^2$ la media e la varianza. Definiamo la media campionaria come
\begin{displaymath}
    \overline{X}:=\frac{X_1+X_2+...+X_n}{n}
\end{displaymath}
Si noti che X è una funzione delle variabili aleatorie $X_1...X_n$. In quanto tale è una statistica, ed è una V.A. Ha senso quindi chiedersi quanto valgano il suo valore atteso e la sua varianza.
\begin{gather*}
    E[\overline{X}]=\mu\\ 
    Var(\overline{X})=\frac{\sigma^2}{n}
\end{gather*}
Ha quindi lo stesso valore atteso della distribuzione da stimare, mentre la sua varianza è ridotta di un fattore n. 
\subsection{Il teorema del limite centrale}
Affrontiamo ora uno dei risultati più notevoli della teoria della probabilità, ossia il \textbf{teorema del limite centrale}. In termini semplicistici, esso afferma che la somma di un numero elevato di V.A. indipendenti tende ad avere distribuzione approssimativamente normale. Di seguente l'enunciato:
Siano $X_1,X_2,...,X_n$ delle variabili aleatorie i.i.d., con media $\mu$ e varianza $\sigma^2$. Allora, se $n$ è grande, la somma $X_1+X_2+...+X_n$ è approssimativamente normale con media $n\mu$ e varianza $n\sigma^2$. Si può anche normalizzare la somma precedente allo scopo di ottenere una normale standard:
\begin{displaymath}
    \frac{X_1+X_2+...+X_n-n\mu}{\sigma\sqrt{n}}\sim \mathcal{N} (0,1)
\end{displaymath}
in realtà sopra al $\sim$ andrebbe un pallino, ad indicare "è approssimativamente distribuito come".
\subsubsection{Distribuzione approssimata della media campionaria}
Sia $X_1,X_2,...,X_n$ un campione proveniente da una popolazione di media $\mu$ e varianza $\sigma^2$. Il teorema appena enunciato ci permette di approssimare la distribuzione della media campionaria. Siccome il prodotto di una V.A. normale per una costante è ancora normale, ne segue che, quando $n$ è grande, $\overline{X}$ è approssimativamente gaussiana. Poiché, inoltre la media campionaria ha valore atteso $\mu$ e deviazione standard $\sigma/\sqrt{n}$, otteniamo che
\begin{displaymath}
    \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{displaymath}
\subsection{La varianza campionaria}
Sia $X_1,X_2,...,X_n$ un campione aleatorio, proveniente da una distribuzione di media $\mu$ e varianza $\sigma^2$. Sia $\overline{X}$ la sua media campionaria. Introduciamo la varianza campionaria come
\begin{displaymath}
    S^2:=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2
\end{displaymath}
La sua radice quadrata $S$ prende invece il nome di deviazione standard campionaria. Volendo calcolare $E[S^2]$ otteniamo che coincide con la varianza della popolazione.
\begin{displaymath}
    E[S^2]=\sigma^2
\end{displaymath}
Cerchiamo ora il valore atteso. Otteniamo che, come ci aspetteremmo, il valore atteso 
\begin{displaymath}
    E[S^2]=\sigma^2
\end{displaymath}
\subsection{Le distribuzioni delle statistiche di popolazioni normali}
Osserviamo i casi in cui la popolazione è di tipo normale. Sia $X_1,...,X_n$ un campione estratto da una distribuzione normale, di media $\mu$ e varianza $\sigma^2$. Le V.A. sono indipendenti e $X_i \sim \mathcal{N}(\mu,\sigma^2)$. Denotiamo come sempre 
\begin{displaymath}
    \overline{X} = \frac{1}{n}\sum^n X_i \hspace{20px}S^2=\frac{1}{n-1}\sum^n(X_i-\overline{X})^2
\end{displaymath}
Vogliamo trovare le loro distribuzioni.
\subsubsection{La distribuzione della media campionaria}
Siccome la somma di normali indipendenti è normale, $\overline{X}$ la è. La sua media e la sua varianza, come nel caso generale, sono $\mu$ e $\sigma^2/n$ e quindi
\begin{displaymath}
    \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim \mathcal{N}(0,1)
\end{displaymath}
è una normale standard.
\subsubsection{La distribuzione congiunta di $\overline{X}$ e $S^2$}
Vogliamo ora derivare la distribuzione di $S^2$. Dimostriamo tramite l'identità che afferma che dati dei numeri $x_1,...,x_n$, posto $y_i:=x_i-\mu$
\begin{displaymath}
    \sum_{i=1}^n(y_i-\overline{y})^2=\sum_{i=1}^n y_i^2 - n\overline{y}^2
\end{displaymath} 
trasformandola in:
\begin{displaymath}
    \sum_{i=1}^n=(x_i-\overline{x})^2 = \sum_{i=1}^n(x_i-\mu)^2-n(\overline{x}-\mu)^2
\end{displaymath}
che la distribuzione seguente è una chi-quadro:
\begin{displaymath}
    (n-1)\frac{S^2}{\sigma^2} \sim \chi_{n-1}^2
\end{displaymath} 
Sappiamo inoltre che dato un campione $X_1,...,X_n$ estratto da una popolazione gaussiana di media $\mu$, se $\overline{X}$ e $S^2$ denotano media e varianza campionaria, allora
\begin{displaymath}
    \frac{\overline{X}-\mu}{S/\sqrt{n}}
\end{displaymath}
Quindi, se si normalizza $\overline{X}$ sottraendo la sua media $\mu$ e dividendo per la sua deviazione standard $\sigma/\sqrt{n}$, si ottiene una normale standard. Se invece dividiamo per $S/\sqrt{n}$ si ha una distribuzione $t$ con $n-1$ gradi di libertà.
\subsection{Campionamento da insiemi finiti}
Consideriamo una popolazione da N elementi. Con il concetto di \textbf{campione aleatorio} si intende la scelta di un sottoinsieme fatto in modo tale che tutti i sottoinsiemi ${N \choose n}$ abbiano la stessa possibilità di essere estratti. Supponiamo ora che una frazione $p$ della popolazione abbia una determinata caratteristica. Avremo quindi $pN$ elementi con la caratteristica, e $(1-p)N$ senza. Definiamo una variabile aleatoria
\begin{displaymath}
    X_i:=\begin{cases}
        1 \mbox{ se l'elemento i possiede la caratteristica}\\ 
        0 \mbox{ se l'elemento non possiede la caratteristica}
    \end{cases}
\end{displaymath}
Consideriamo la somma di queste V.A.
\begin{displaymath}
    X:=X_1+X_2+...+X_n
\end{displaymath}
Passiamo ora ad analizzare le probabilità associate alle statistiche $X$ e $\overline{X}$. Notiamo innanzitutto che $P(X_i = 1) = p$. Notiamo che le variabili $X_i$ sono bernoulliane \textbf{non indipendenti}. Ciò è ovvio perché, avendo estratto un elemento positivo, le probabilità della seguente estrazione diminuiscono:
\begin{displaymath}
    P(X_2 = 1 | X_1=1) = \frac{pN-1}{N-1}
\end{displaymath}
ci sono infatti $pN-1$ elementi positivi rimasti su $N-1$. È facile notare che se $n$ è grande, la probabilità cambia poco e quindi si tende all'indipendenza. Siccome la somma di bernoulliane indipendenti è una binomiale, otteniamo che con $n$ elevato, $X:=\sum_i X_i$ è una binomiale di parametri $n$ e $p$.
\section{Stima parametrica}
Consideriamo un campione aleatorio $X_1,...,X_n$ estratto da una distribuzione $F_0$ che dipende da un vettore di parametri incogniti $\theta$. Potrebbe ad esempio trattarsi di V.A. di Poisson, delle quali ignoriamo il valore di $\lambda$. Ci occupiamo ora di trovare dei metodi per stimare questi parametri incogniti, sia in maniera puntuale, che tramite i cosiddetti \textit{intervalli di confidenza}.  
\subsection{Stimatori di massima verosimiglianza (ML)}
Una qualunque statistica che cerchi di dare una stima di un parametro $\theta$ si dice \textbf{stimatore di $\theta$}. Ad esempio, la media campionaria costituisce lo stimatore abituale della media $\mu$ di una distribuzione. Consideriamo delle variabili aleatorie $X_1,...,X_n$ la cui distribuzione congiunta sia nota a meno di un parametro incognito $\theta$. Per esempio, queste variabili hanno media $\theta$ incognita. Il nostro compito è quello di stimare $\theta$ partendo dai valori osservati. Utilizzeremo quindi uno \textit{stimatore di massima verosimiglianza}, ottenibile con il ragionamento seguente. Denotiamo con $f(x_1,...,x_n|\theta)$ la funzione di massa congiunta di $X_1,...,X_n$, mostrando esplicitamente che $f$ dipende da $\theta$. Se interpretiamo $f$ come la verosimiglianza che si realizzi la n-upla di dati $x_1,...,x_n$ quando $\theta$ è il vero valore assunto dal parametro, sembra ragionevole adottrare come stima il valore che rende massima la funzione. Conviene spesso usare il fatto che il logaritmo della suddetta ha il massimo nello stesso punto. 
\subsubsection{Stimare la distribuzione dei tempi di vita}
Prendiamo una popolazione i cui elementi hanno un tempo di vita casuale. Sia X il numero di anni che vivrà una persona a caso nata oggi. Indichiamo con $\lambda_i$ la probabilità che una persona nata oggi, e sopravvisuta per i primi $i-1$ anni di vita, muoia durante l'i-esimo anno di vita. 
\begin{displaymath}
    \lambda_i:=P(X=i|X>i-1)=\frac{P(X=i)}{P(X>i-1)}
\end{displaymath}
poi, con $s_i=1-\lambda_i$, indichiamo la probabilità che sopravviva anche all'anno i. Le quantità $\lambda_i$ e $s_i$ sono dette rispettivamente funzione di rischio e funzione di sopravvivenza di un individuo che entra nel suo i-esimo anno di vita. 
Notiamo che 
\begin{displaymath}
    P(X=n) = P(X>n-1)\lambda_n = s_1s_2...s_{n-1}\lambda_n
\end{displaymath}
È perciò possibile stimare la funzione di massa di X a partire dalle stime delle quantità $s_1,...,s_n$, ottenibili da un campione di individui, considerando quelli che sono entrati nel loro i-esimo anno di vita un anno fa, e denotando con $\hat{s_i}$ la frazione di essi ancora in vita oggi.
\subsection{Intervalli di confidenza}
Sia $X_1,...,X_n$ un campione estratto da una popolazione normale di media incognita $\mu$ e varianza nota $\sigma^2$. Abbiamo in precedenza dimostrato che $\overline{X}:=\sum_i X_i/n$ è lo stimatore ML per $\mu$. A volte, però, vorremmo un intervallo, piuttosto che un singolo punto. Per ottenerlo, dobbiamo fare uso della distribuzione di probabilità dello stimatore. Ricordando che $\overline{X}$ è una normale, sappiamo che
\begin{displaymath}
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)    
\end{displaymath}
Perciò, 
\begin{gather*}
    P\left(-1.96<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<1.96\right)\rightarrow
    P\left(\overline{X}-1.96\frac{\sigma}{\sqrt{n}}<\mu<\overline{X}+1.96\frac{\sigma}{\sqrt{n}}\right)
\end{gather*}
il 95\% delle volte $\mu$ starà a una distanza non superiore a $1.96\sigma/\sqrt{n}$. Stiamo quindi affermando che nel 95\% dei casi, la media apparterrà all'intervallo $\left(-1.96\frac{\sigma}{\sqrt{n}}, +1.96\frac{\sigma}{\sqrt{n}}\right)$.
A volte ci interessa trovare un intervallo \textit{unilaterale}. Per trovarlo, notiamo che se $Z$ è $\mathcal{N}(0,1)$, allora
\begin{displaymath}
    0.95\approx P(Z< 1.645) \approx P\left(\overline{X}-1.645\frac{\sigma}{\sqrt{n}}<\mu\right)
\end{displaymath}
Avremo quindi un intervallo unilaterale destro con il 95\% di confidenza 
\begin{displaymath}
    \left(\overline{x}-1.645\frac{\sigma}{\sqrt{n}},\infty\right)
\end{displaymath}
Possiamo ottenere livelli di confidenza per ogni livello. Ricordiamo infatti i numeri $z_\alpha$ tali che $P(Z>z_\alpha) = \alpha$. Questo implica che per ogni $\alpha \in (0,1)$ 
\begin{displaymath}
    P(-z_{\alpha/2}<Z<z_{\alpha/2})=1-\alpha
\end{displaymath}
Da cui deduciamo la formula generale 
\begin{displaymath}
    P\left(\overline{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu <      \overline{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) = 1-\alpha
\end{displaymath}
Ugualmente, per gli intervalli unilaterali ci basterà usare $z_\alpha$ e non $z_{\alpha/2}$.
\subsubsection{Intervalli di confidenza per la media di una normale, con varianza incognita}
Abbiamo un campione $X_1,...,X_n$ di una popolazione $\mathcal{N}(\mu, \sigma^2)$ con entrambi i parametri incogniti. Siccome la deviazione standard non è più nota, non possiamo più basarci sul fatto che $\sqrt{n}(\overline{X}-\mu)/\sigma$ è una normale standard. Tuttavia, usando la varianza campionaria $S^2$, otteniamo una V.A. di tipo $t$ con $n-1$ gradi di libertà. Allora, poiché la densità delle distribuzioni t è simmetrica rispetto a 0 come quella della normale standard, avremo per $\alpha \in (0,1/2)$
\begin{displaymath}
    P\left(\overline{X}-t_{\frac{\alpha}{2}, n-1}\frac{S}{\sqrt{n}} < \mu <  \overline{X}+t_{\frac{\alpha}{2}, n-1}\frac{S}{\sqrt{n}}\right)
\end{displaymath}
\subsubsection{Intervalli di confidenza per la varianza di una distribuzione normale}
Abbiamo il solito campione proveniente da una normale, con entrambi i parametri incogniti. Possiamo costruire intervalli di confidenza per $\sigma^2$ basandoci sul fatto che
\begin{displaymath}
    (n-1)\frac{S^2}{\sigma^2} \sim \chi_{n-1}^2
\end{displaymath}
Infatti:
\begin{displaymath}
    P\left(\frac{(n-1)S^2}{\chi_{\frac{\alpha}{2},n-1}} < \sigma^2 <    \frac{(n-1)S^2}{\chi_{1-\frac{\alpha}{2},n-1}}\right)
\end{displaymath}
\subsection{Stime per la differenza tra le medie di due normali}
Abbiamo due campioni $X_1,...,X_n$ e $Y_1,...,Y_n$ con parametri $(\mu_1,\sigma^2_1)$ e $(\mu_2,\sigma^2_2)$. Vogliamo stimare $(\mu_1-\mu_2)$. Dal momento che i due stimatori delle medie $\overline{X}$ e $\overline{Y}$ sono normali, assumiamo che $\overline{X}-\overline{Y}$ sia una normale. Inoltre,
\begin{displaymath}
    \overline{X}-\overline{Y} \sim \left(\mu_1-\mu_2, \frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}\right)
\end{displaymath}
che abbiamo trovato sfruttando le proprietà di valore atteso e varianza. 
Ipotizzando di conoscere $\sigma_1^2$ e $\sigma_2^2$, abbiamo che
\begin{displaymath}
    \frac{\overline{X}-\overline{Y} - (\mu_1-\mu_2)}{\sqrt{\sigma_{1}^2/n+\sigma_2^2/m}} \sim \mathcal{N}(0,1)
\end{displaymath}
Usando i soliti passaggi, otteniamo
\begin{displaymath}
    P\left(\overline{X}-\overline{Y}-z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}<\mu_1-\mu_2< \overline{X}-\overline{Y}+z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}\right) = 1-\alpha
\end{displaymath}
\textit{Dai, sono sicuro che all'esame te la ricorderai. Senz'altro.} 
\subsection{Intervalli di confidenza approssimati per la media di una Bernoulliana}
Consideriamo una popolazione di oggetti, ognuno dei quali indipendentemente da tutti gli altri soddisfa certi requisiti con probabilità incognita $p$. Nel caso vengano testati $n$ di questi oggetti, rilevando quanti di essi raggiungono tali requisiti, come posso arrivare a un intervallo di confidenza per $p$? Se X denota quanti oggetti, sugli $n$ testati, soddisfano i requisiti, è intuitivo che essa sia una binomiale di parametri $n$ e $p$. Quindi se $n$ è sufficientemente elevato, essa si approssima a una normale.
\begin{displaymath}
    \frac{X-np}{\sqrt{np(1-p)}} \sim \mathcal{N}(0,1)
\end{displaymath}
Quindi, come sempre
\begin{displaymath}
    P\left(-z_{\frac{\alpha}{2}} < \frac{x-np}{\sqrt{np(1-p)}} < z_{\frac{\alpha}{2}}\right)
\end{displaymath}

Tale regione non è però un intervallo. Se vogliamo ottenere un intervallo di confidenza vero e proprio, denotiamo con $\hat{p}:=X/n$ la frazione di elementi che soddisfa i requisiti. Sappiamo che $\hat{p}$ è lo stimatore di massima verosimiglianza di $p$, e ne è una buona approssimazione. Per questo, 
\begin{displaymath}
    \sqrt{n\hat{p}(1-\hat{p})} \approx \sqrt{np(1-p)}
\end{displaymath}
Quindi, ne deduciamo che 
\begin{displaymath}
    \frac{X-np}{\sqrt{n\hat{p}(1-\hat{p})}}\sim \mathcal{N}(0,1)
\end{displaymath}
Che ci permette di ottenere un intervallo di confidenza
\begin{displaymath}
    P\left(-z_{\frac{\alpha}{2}} < \frac{X-np}{\sqrt{n\hat{p}(1-\hat{p})}} <  z_{\frac{\alpha}{2}}\right) = 1-\alpha 
\end{displaymath}
\subsection{Intervalli di confidenza per la media di una distribuzione esponenziale}
Consideriamo un campione $X_1,...,X_n$ di variabili aleatorie esponenziali i.i.d. tutte con media $\theta$ incognita. Lo stimatore ML per la media è il solito. Per ottenere gli intervalli di confidenza, ricordiamo che $\sum_{i=1}^n X_i$ ha distribuzione gamma con parametri $n$ e $1/\theta$. Sappiamo che
\begin{displaymath}
    \frac{2}{\theta}\sum_{i=1}^n X_i \sim \chi_{2n}^2
\end{displaymath}
Quindi per ogni $\alpha \in (0,1)$
\begin{displaymath}
    P\left(\frac{2\sum_{i=1}^n X_i}{\chi_{\frac{\alpha}{2},2n}} < \theta <   \frac{2\sum_{i=1}^n X_i}{\chi_{1-\frac{\alpha}{2},2n}}  \right)
\end{displaymath} 
\subsection{Valutare l'efficienza degli stimatori puntuali}
Sia $X:=(X_1,...,X_n)$ un campione casuale estratto da una popolazione di distribuzione nota eccetto che per un parametro incognito $\theta$, e sia $d=d(X)$ uno stimatore di $\theta$. Come possiamo valutare l'efficacia di questo stimatore?
Il criterio che utilizziamo è il cosiddetto \textit{errore quadratico medio} dello stimatore, che per definizione è
\begin{displaymath}
    r(d,\theta) := E[(d(X)-\theta)^2]
\end{displaymath}
Sarebbe ideale avere un singolo stimatore $d$ che minimizzasse $r(d,\theta)$ per tutti i valori di $\theta$, ma ciò non è possibile. Cerchiamo quindi, ad esempio, stimatori che minimizzino $r(d,\theta)$ e siano non distorti. Definiamo
\begin{displaymath}
    b_\theta (d) := E[d(X)] - \theta
\end{displaymath}
il \textbf{bias} di $d$ come stimatore di $\theta$. Se esso è nullo, diremo che $d$ è uno stimatore corretto, o \textit{non distorto}. In altri termini, uno stimatore è corretto se il suo valore atteso coincide con il parametro che esso deve stimare.
\subsection{Stimatori Bayesiani}
Vista l'indeterminazione del parametro incognito $\theta$, in alcune situazioni può essere ragionevole considerarlo assumere la forma di una V.A., facendo diventare il valore vero del parametro da stimare il numero realizzato dalla variabile. Questo approccio viene detto \textbf{bayesiano}, ed è di norma giustificato quando, prima di osservare gli esiti del campione di dati $X_1,...,X_n$, abbiamo delle informazioni sui possibili valori di $\theta$ e la loro plausibilità. Se queste informazioni assumono la forma di una distribuzione di probabilità, questa prende appropriatamente il nome di distribuzione a priori per $\theta$. Supponiamo allora di poter esprimere le nostre considerazioni a priori su $\theta$ nella forma di una distribuzione continua, con densità di probabilità $p(\theta)$; osserviamo i valori di un campione di dati la cui distribuzione dipende da $\theta$, e denotiamo con $f(x|\theta)$ la funzione di likelihood. Si tratta quindi della funzione di massa di probabilità/densità di probabilità, che esprime la plausibilità che uno dei dati sia uguale a $x$ quando $\theta$ è il valore del parametro. Se i valori osservati sono $X_i=x_i$ allora la probabilità condizionale di $\theta$ è data da
\begin{displaymath}
    f(\theta|x_1,...,x_n) = \frac{f(x_1,...,x_n,\theta)}{f(x_1,...,x_n)}
\end{displaymath}  
La densità condizionale $f(\theta|x_1,...,x_n)$ è detta densità di probabilità a posteriori. Se conosciamo la distribuzione, la migliore stima di una V.A. è la sua media. Quindi, per stimare $\theta$ calcoliamo la media della distribuzione a posteriori $f(\theta|x_1,...,x_n)$. Tale stimatore è detto \textbf{stimatore bayesiano}, si indica con $E[\theta|X_1,...,X_n]$ e il suo valore si calcola nel modo usuale:
\begin{displaymath}
    E[\theta|X_1,...,X_n] = \int_{-\infty}^\infty \theta f(\theta | x_1,...,x_n) d\theta
\end{displaymath}
\section{Verifica delle ipotesi}
Abbiamo ora un campione aleatorio che ci è noto tranne che per uno o più parametri incogniti. Vogliamo verificare qualche ipotesi che li coinvolga. A priori, non sappiamo se l'ipotesi è vera o meno; vogliamo solo verificare se i dati raccolti la escludono, o no.
\subsection{Livelli di significatività}
Consideriamo una popolazione avente distribuzione $F_\theta$ che dipende da un parametro incognito $\theta$, e supponiamo di voler verificare una qualche ipotesi su $\theta$, che chiameremo \textit{ipotesi nulla} e denoteremo con $H_0$. Parleremo di \textit{ipotesi semplice}, quando essa caratterizza completamente la distribuzione, di \textit{ipotesi composta} quando non avviene. Supponiamo di voler disporre di un campione aleatorio $X_1,...,X_n$ proveniente da questa popolazione, e di volerlo utilizzare per eseguire una verifica su una certa ipotesi nulla $H_0$. Siccome dobbiamo valutare esclusivamente sugli $n$ valori dei dati, il test sarà definito da una regione C nello spazio a $n$ dimensioni, con l'intesa che se il vettore $(X_1,...,X_n)$ cade all'interno di C l'ipotesi viene rifiutata, mentre viene accettata altrimenti. Una regione C con queste caratteristiche viene detta \textit{regione critica} del test. Per esempio, se volessimo verificare che una popolazione gaussiana di varianza 1 abbia media 1, produrremmo la regione critica seguente:
\begin{displaymath}
    C=\left\{(X_1,...,X_n):\left|1-\frac{1}{n}\sum_{i=1}^n X_i\right| > \frac{1.96}{\sqrt{n}} \right\}
\end{displaymath}
Possono esistere due tipi di errori in questo procedimento:
\begin{itemize}
    \item \textit{Errori di prima specie}: quelli che gli informatici (\textit{gente a posto}) chiamano \textbf{false negative}.
    \item \textit{Errori di seconda specie}: quelli che gli informatici chiamano \textbf{false positive}.
\end{itemize}
Per bilanciare tra i due, specifichiamo un livello $\alpha$, detto \textbf{livello di significatività}, e imponendo che il test abbia la proprietà che quando l'ipotesi $H_0$ è vera, la probabilità che venga rifiutata non possa superare $\alpha$. Esso viene fissato solitamente attorno a 0.1,0.05, 0.005. 
\subsection{La verifica delle ipotesi sulla media di una popolazione normale}
\subsubsection{Il caso in cui la varianza è nota}
Supponiamo il solito campione aleatorio con parametri $\mu$ e $\sigma^2$, con varianza nota e media incognita. Vogliamo verificare l'ipotesi nulla
\begin{displaymath}
    H_0:\mu = \mu_0
\end{displaymath}
contro l'ipotesi alternativa
\begin{displaymath}
    H_1:\mu \neq \mu_0
\end{displaymath}
Siccome lo stimatore puntuale normale per $\mu$ è $\overline{X}:=\frac{1}{n}\sum_1^n X_i$, è chiaro che dovremo accettare $H_0$ quando $\mu_0$ non è troppo lontano da $\overline{X}$. La regione critica sarà quindi
\begin{displaymath}
    C=\left\{(X_1,...,X_n) |\overline{X}-\mu_0|>c \right\}
\end{displaymath}
scegliendo opportunamente la costante $c$. Se vogliamo un livello di significatività $\alpha$, dobbiamo individuare quel valore di $c$ che rende pari ad $\alpha$ la probabilità di errori di prima specie. Ciò significa che $c$ deve soddisfare la relazione seguente:
\begin{displaymath}
    \alpha=P_{\mu_0}(|\overline{X}-\mu_0|>c)
\end{displaymath}
dove $P_{\mu_0}$ sta ad intendere che la probabilità viene calcolata con l'assunzione che $\mu=\mu_0$. Infatti la definizione di errore di prima specie significa che l'errore ci porta a rifiutare un'ipotesi in realtà vera. Quando però $\mu=\mu_0$, sappiamo che $\overline{X}$ ha distribuzione normale con media $\mu$ e varianza $\sigma^2/n$, e quindi se Z denota una V.A. normale std, allora
\begin{displaymath}
    \frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}\sim Z
\end{displaymath}
Otteniamo allora 
\begin{displaymath}
    P\left(|Z|>\frac{c\sqrt{n}}{\sigma} \right) = 2P\left(Z>\frac{c\sqrt{n}}{\sigma} \right) 
\end{displaymath}
Ed in definitiva, con il test con livello di significatività $\alpha$
\begin{gather*}
    \mbox{si rifiuta }H_0\mbox{ se }\left|\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}   \right| > z_{\frac{\alpha}{2}}\\ 
    \mbox{si accetta }H_0\mbox{ se }\left|\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}   \right| \le z_{\frac{\alpha}{2}} 
\end{gather*}
\subsubsection{I test unilaterali}
Nel verificare l'ipotesi nulla $\mu=\mu_0$, abbiamo costruito un test che porta ad un rifiuto quando $\overline{X}$ è lontana da $\mu_0$, ovvero, valori di $\overline{X}$ troppo bassi o troppo elevati rispetto a $\mu_0$ sembrano smentire che $\mu$ sia proprio uguale a $\mu_0$. Cosa accade invece quando $\mu$ può essere solo maggiore a $\mu_0$, quando non sono uguali? Ovvero, cosa occorre fare se l'ipotesi alternativa a $H_0 : \mu = \mu_0$ è $H_1:\mu>\mu_0$? Dovremmo quindi rifiutare l'ipotesi nulla quando $\overline{X}$, lo stimatore di $\mu$, è molto maggiore di $\mu_0$, e quindi la regione critica dovrebbe essere del tipo seguente:
\begin{displaymath}
    C := \{(X_1,...,X_n):\overline{X}-\mu_0 >c \}
\end{displaymath}
per una scelta opportuna della costante c. In particolare, siccome la probabilità di rifiuto dovrebbe essere $\alpha$ quando $H_0$ è vera, occorre che $c$ soddisfi la relazione
\begin{displaymath}
    P_{\mu_0}(\overline{X}-\mu_0 >c) = \alpha
\end{displaymath}
Di nuovo, poiché supponiamo $\mu = \mu_0$, esiste una statistica Z normale standard così definita:
\begin{displaymath}
    Z:=\frac{\overline{X}- \mu_0}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)
\end{displaymath}
Otteniamo quindi
\begin{displaymath}
    P\left(Z >  \frac{c\sqrt{n}}{\sigma/\sqrt{n}}\right)
\end{displaymath}
E, finalmente, ricaviamo c:
\begin{displaymath}
    c=z_\alpha\frac{\sigma}{\sqrt{n}}
\end{displaymath}
Quella trovata è una regione critica \textbf{unilaterale}.
\subsubsection{Quando la varianza non è nota: il test t}
Fino ad ora abbiamo supposto di conoscere la varianza. Spesso non è così. Possiamo però sfruttare i calcoli precedenti, utilizzano, al posto della deviazione standard, il suo stimatore: la deviazione standard campionaria.
\begin{displaymath}
    S := \sqrt{\frac{1}{n-1}\sum_1^n(X_i-\overline{X})^2}
\end{displaymath}
rifiutando l'ipotesi nulla quando
\begin{displaymath}
    \left| \frac{\overline{X}- \mu_0}{S/\sqrt{n}}    \right|
\end{displaymath}
è troppo grande. Per definire questa "grandezza" sfruttiamo il fatto che questa assume una distribuzione $t$ con $n-1$ gradi di libertà. Denotiamola con T, e imponiamo che la probabilità di errori di prima specie sia $\alpha$:
\begin{displaymath}
    P_{\mu_0}\left(-c \le \frac{\overline{X}- \mu_0}{S/\sqrt{n}} \le c\right)
\end{displaymath}
Per ricavare c, sappiamo che la distribuzione $t$ è simmetrica rispetto a 0, per cui $P(T > c) = \frac{\alpha}{2}$ e quindi $C=t_{\frac{\alpha}{2},n-1}$.
In definitiva:
\begin{gather*}
    \mbox{si rifiuta } H_0\mbox{ se }\left|\frac{\overline{X}-\mu_0}{S/\sqrt{n}}     \right| > t_{\frac{\alpha}{2},n-1} \\ 
    \mbox{si accetta } H_0\mbox{ se }\left|\frac{\overline{X}-\mu_0}{S/\sqrt{n}}     \right| \le t_{\frac{\alpha}{2},n-1}
\end{gather*}
\subsection{Verifica se due popolazioni normali hanno la stessa media}
Vogliamo verificare se, ad esempio, due diversi approcci ad un problema hanno ricevuto risposte simili. Come fare? Possiamo verificare se "hanno la stessa media".
\subsubsection{Se le varianze sono note}
Abbiamo due campioni $X_1,...,X_n$ e $Y_1,...,Y_n$ indipendenti, provenienti da popolazioni normali di medie incognite $\mu_x$ e $\mu_y$, e varianze note $\sigma_x^2, \sigma_y^2$. Vogliamo verificare l'ipotesi 
\begin{displaymath}
    H_0:\mu_x = \mu_y \mbox{  contro  }H_1:\mu_x\neq\mu_y
\end{displaymath} 
Sfruttando i due stimatori delle medie, sembra ragionevole rifiutare l'ipotesi quando $\overline{X}-\overline{Y}$ è molto lontano da 0. Troviamo un valore di $c$ adatto, ricordando che
\begin{displaymath}
    \overline{X}-\overline{Y} \sim \mathcal{N}\left(\mu_x-\mu_y , \frac{\sigma_x^2}{n}+\frac{\sigma_y^2}{m}\right)
\end{displaymath} 
Per cui, per avere una normale standard:
\begin{displaymath}
    \frac{\overline{X}- \overline{Y} -(\mu_x-\mu_y)}{\sqrt{\sigma_x^2/n+\sigma_y^2/m}} \sim \mathcal{N}(0,1)
\end{displaymath}
Quindi, quando $H_0$ è vera, la statistica del test $\frac{\overline{X}- \overline{Y}}{\sqrt{\sigma_x^2/n+\sigma_y^2/m}}$ ha distribuzione normale standard.
Per cui, come sempre
\begin{gather*}
    \mbox{si rifiuta }H_0\mbox{ se }\frac{|\overline{X}- \overline{Y}|}{\sqrt{\sigma_x^2/n+\sigma_y^2/m}} > z_{\frac{\alpha}{2}}\\
    \mbox{si accetta }H_0\mbox{ se }\frac{|\overline{X}- \overline{Y}|}{\sqrt{\sigma_x^2/n+\sigma_y^2/m}} \le z_{\frac{\alpha}{2}}
\end{gather*}
\subsubsection{Se le varianze non sono note, ma si suppongono uguali}
Con il caso di cui sopra, non sappiamo ora le varianze, ma sappiamo che sono uguali. Calcoliamo le due varianze campionarie, e ricordiamo che
\begin{displaymath}
    \frac{\overline{X}- \overline{Y} -(\mu_x-\mu_y)}{S_p\sqrt{1/n+1/m}} \sim t_{n+m-2}
\end{displaymath}
dove $S_p^2$ è lo stimatore \textit{pooled} di $\sigma^2$, definito da
\begin{displaymath}
    S_p^2 := \frac{(n-1)S_x^2+(m-1)S_y^2}{n+m-2}
\end{displaymath}
Perciò, quando $H_0$ è vera, la statistica del test 
\begin{displaymath}
    T:=\frac{\overline{X}- \overline{Y}}{S_p\sqrt{1/n+1/m}} \sim t_{n+m-2}
\end{displaymath}
ha distribuzione t con $n+m-2$ gradi di libertà. Ne segue che possiamo verificare l'ipotesi come
\begin{gather*}
    \mbox{si rifiuta }H_0\mbox{ se }|T|>t_{\frac{\alpha}{2},n+m-2}\\ 
    \mbox{si accetta }H_0\mbox{ se }|T| \le t_{\frac{\alpha}{2},n+m-2}
\end{gather*}
Potremmo anche determinare il p-dei-dati $= 2P(T_{n+m-2} \ge |v|)$, dove $T$ è una $t$ di Student con $n+m-2$ gradi di libertà. 
Se invece volessivo verificare un'ipotesi unilaterale come 
\begin{displaymath}
    H_0 : \mu_x \le \mu_y \mbox{ contro }H_1:\mu_x>\mu_y
\end{displaymath}
allora $H_0$ deve essere rifiutata per valori elevati di T, e in particolare il test ha livello di significatività $\alpha$ quando 
\begin{displaymath}
    \mbox{si rifiuta }H_0\mbox{ se } T>t_{\alpha,n+m-2}\\ 
    \mbox{si accetta }H_0\mbox{ se } T\le t_{\alpha,n+m-2}
\end{displaymath}
Se $v$ è il valore assunto dalla statistica $T$, allora il p-dei-dati corrispondente è $P(T_{n+m-2}\ge v)$.
\subsubsection{Se le varianze sono ignote e diverse}
In questo caso, usiamo gli stimatori al posto delle deviazioni standard. C'è una sola condizione a ciò: $n$ e $m$ devono essere molto elevati, in modo che la statistica 
\begin{displaymath}
    \frac{\overline{X}- \overline{Y}}{\sqrt{S_x^2/n+S_y^2/m}}
\end{displaymath}
sia approssimativamente normale.
Riassumendo:
\begin{gather*}
    \mbox{si accetta }H_0\mbox{ se }-z_{\frac{\alpha}{2}} \le \frac{\overline{X}- \overline{Y}}{\sqrt{S_x^2/n+S_y^2/m}}\le z_{\frac{\alpha}{2}}\\ 
    \mbox{si accetta }H_0\mbox{ in tutti gli altri casi}
\end{gather*}
\subsection{La verifica di ipotesi sulla varianza di una popolazione normale}
Abbiamo il solito campione, con media $\mu$ e varianza $\sigma^2$, entrambe incognite. Vogliamo verificare l'ipotesi
\begin{displaymath}
    H_0:\sigma^2 = \sigma_0^2 \mbox{ contro }H_1: \sigma^2 \neq \sigma_0^2
\end{displaymath}
per un valore di $\sigma_0^2$ fissato. Per ottenere un test, ricordiamo che $(n-1)\frac{S^2}{\sigma^2}$ ha distribuzione chi-quadro con $n-1$ gradi di libertà. Perciò, 
\begin{displaymath}
    P_{H_0}\left(\chi^2_{1-\frac{\alpha}{2},n-1} \le  (n-1)\frac{S^2}{\sigma_0^2} \le       \chi^2_{\frac{\alpha}{2},n-1} \right) = 1-\alpha
\end{displaymath}
e in definitiva
\begin{gather*}
    \mbox{si accetta }H_0\mbox{ se }\chi^2_{1-\frac{\alpha}{2},n-1} \le  (n-1)\frac{S^2}{\sigma_0^2} \le       \chi^2_{\frac{\alpha}{2},n-1}\\ 
    \mbox{si rifiuta negli altri casi}
\end{gather*}
Il p-dei-dati è il seguente
\begin{displaymath}
    \mbox{p-dei-dati}=2\mbox{min}\{P(\chi_{n-1}^2 \le c), 1-P(\chi_{n-1}^2 \le c)\}
\end{displaymath}
\subsubsection{Verificare se due popolazioni normali hanno la stessa varianza}
Abbiamo i soliti due campioni, provenienti da popolazioni di parametri incogniti $\mu_x, \sigma^2_x$ e $\mu_y, \sigma^2_y$, e verifichiamo 
\begin{displaymath}
    H_0:\sigma^2_x=\sigma^2_y \mbox{ contro }H_1: \sigma_x^2 \neq \sigma_y^2
\end{displaymath}
Definiamo le varianze campionarie come al solito, e sappiamo che $(n-1)S_x^2/\sigma_x^2$ è una chi-quadro con $n-1$ gradi di libertà.
Quindi, $(S_x^2/\sigma_x^2)/(S_y^2/\sigma_y^2)$ ha distribuzione F con parametri $n-1$ e $m-1$, e quando $H_0$ è vera,
\begin{displaymath}
    \frac{S_x^2}{S_y^2}\sim F_{n-1,m-1}
\end{displaymath}
da cui si deduce che 
\begin{displaymath}
    P_{H_0}\left(F_{1-\frac{\alpha}{2},n-1,m-1} \le     \frac{S_x^2}{S_y^2} \le  F_{\frac{\alpha}{2},n-1,m-1}\right) = 1-\alpha
\end{displaymath}
Si accetta quindi $H_0$ se $F_{1-\frac{\alpha}{2},n-1,m-1} \le     \frac{S_x^2}{S_y^2} \le  F_{\frac{\alpha}{2},n-1,m-1}$. In alternativa, si calcola il valore $v$ assunto dalla statistica del test, $\frac{S_x^2}{S_y^2}$. Il p-dei-dati è uguale a $2$min$\{P(F_{n-1,m-1}\le v), 1-P(F_{n-1,m-1}\le v)\}$.
\subsection{La verifica di ipotesi su una popolazione di Bernoulli}
La distribuzione binomiale compare frequentemente nei problemi dell'ingegneria. Un esempio tipico è un processo produttivo che restituisce parti difettose o accettabili. Sappiamo che i pezzi saranno accettabili con probabilità $p$. Consideriamo un'ipotesi
\begin{displaymath}
    H_0:p \le p_0 \mbox{ contro }H_1: p>p_0
\end{displaymath}
dove $p_0$ è un valore assegnato. Se denotiamo con X il numero di pezzi difettosi di un campione di $n$, dobbiamo certamente rifiutare $H_0$ quando X è troppo grande. Per calcolare quanto grande, notiamo che quando $H_0$ è vera
\begin{displaymath}
    P_{H_0}(X\ge k) \le \sum_k^n {n \choose i} p_0^i (1-p_0)^{n-i}
\end{displaymath}
Per verificare le ipotesi ad un livello $\alpha$, si deve rifiutare quando $X\ge k^*$ con k il numero più piccolo intero $k$ tale che $\sum_{i=k}^n {n \choose i} p_0^i(1-p_0)^{n-i}\le \alpha$. Un modo migliore per implementare il test sarebbe ricavare prima il valore $x$ della statistica del test, e poi calcolare il p-dei-dati come segue:
\begin{displaymath}
    \mbox{p-dei-dati} = \sum_{i=x}^n {n \choose i} p_0^i (1-p_0)^{n-i}
\end{displaymath}
\subsubsection{Verificare se due popolazioni di Bernoulli hanno lo stesso parametro}
Immaginiamo di voler confrontare due diversi metodi di fabbricazione per transistor. Indichiamo con $p_1$ e $p_2$ le probabilità, incognite, che un pezzo prodotto con i metodi 1 e 2 sia difettoso. Raccogliamo poi i campioni di numerosità $n_1$ e $n_2$ e indichiamo con $X_1$ e $X_2$ i pezzi difettosi trovati. Otterremo un'ipergeometrica di parametri $n_1,n_2,k$, con $k=X_1+X_2$. Perciò, 
\begin{displaymath}
    P_{H_0}(X_1=i|X_1+X_2=k)=\frac{{n_1 \choose i}{n_2 \choose k-i}}{{n_1+n_2 \choose k}}
\end{displaymath}
Volendo realizzare il test 
\begin{displaymath}
    H_0: p_1=p_2\mbox{ contro }H_1: p_1 \neq p_2
\end{displaymath}
si osserva quanto valgono $X_1=x_1$ e $X_2=x_2$ e si calcola la somma $k=x_1+x_2$. Denotata quindi X una V.A. ipergeometrica, se 
\begin{gather*}
    \mbox{si rifiuta } H_0\mbox{ se }P(X\le x_1)<\frac{\alpha}{2}\mbox{ o }P(X \ge x_1) < \frac{\alpha}{2}\\ 
    \mbox{si accetta altrimenti}
\end{gather*}
Il p-dei-dati è pari a $2$min$\{P(X\le x_1),P(X \ge x_1)\}$.
\subsection{La verifica di ipotesi sulla media di una distribuzione di Poisson}
Sia X una Poisson con media $\lambda$, e supponiamo di voler confrontare le ipotesi
\begin{displaymath}
    H_0:\lambda=\lambda_0\mbox{ contro }H_1:\lambda\neq\lambda_0
\end{displaymath}
Se $x$ è il valore osservato per X, un test con controllo di significatività $\alpha$ deve rifiutare l'ipotesi nulla se 
\begin{displaymath}
    P_{\lambda_0}(X\ge x)\le \frac{\alpha}{2}\mbox{ o se }P_{\lambda_0}(X\le x)\le \frac{\alpha}{2}
\end{displaymath}
dove con $P_{\lambda_0}$ intendiamo la probabilità calcolata assumendo che X abbia media $\lambda_0$. Il p-dei-dati è equivalente a $2$min$\{P_{\lambda_0}(X \le x), P(X\ge x)\}$.
\subsubsection{Testare la relazione tra i parametri di due Poisson}
Siano $X_1$ e $X_2$ due V.A. di Poisson indipendenti, con medie rispettivamente $\lambda_1$ e $\lambda_2$, e di voler vagliare l'ipotesi
\begin{displaymath}
    H_0:\lambda=c\lambda_1\mbox{ contro }H_1:\lambda_2 \neq c\lambda_1
\end{displaymath}
per una costante $c$ assegnata. Il test che costruiamo è di tipo condizionale, ed è basato sul fatto che la distribuzione di $X_1$ condizionata al valore della somma di $X_1+X_2$, è binomiale. In particolare, la binomiale avrà parametri $(X_1+X_2)$ e $\lambda_1/(\lambda_1+\lambda_2)$. Dovremo allora rifiutare $H_0$ qualora
\begin{displaymath}
    P(Y\ge x_1)\le\frac{\alpha}{2}\mbox{ oppure }P(Y\le x_1)\le \frac{\alpha}{2}
\end{displaymath}
dove Y è la binomiale.
\section{Regressione}
Spesso vogliamo determinare le relazioni tra variabili, piuttosto che informazioni sulle stesse. Sarà quindi necessario avere più ingressi $x_1,...,x_r$, ed una sola uscita $Y$. Il modello suppone che la risposta sia in funzione degli ingressi; per questo Y è anche detta variabile dipendente, mentre le $x$, indipendenti. La più semplice relazione che è possibile immaginare è quella lineare; essa si presenta quando per delle opportune costanti $\beta_0,...,\beta_r$ vale l'equazione
\begin{displaymath}
    Y=\beta_0+\beta_1 x_1+\beta_2x_2+...+\beta_rx_r
\end{displaymath}
Se la relazione fosse questa saremmo a posto. Visto che non la è, dovremo sommare alla suddetta un errore casuale $e$. Possiamo infatti esprimerla anche come 
\begin{displaymath}
    E[Y|\textbf{x}]=\beta_0+\beta_1 x_1+\beta_2x_2+...+\beta_rx_r
\end{displaymath}
Infine, enunciamo l'\textbf{equazione di regressione lineare}, che con i suoi \textbf{coefficienti di regressione} $\beta$ si dice semplice se $r=1$, multipla altrimenti.
\begin{displaymath}
    Y=\beta_0+\beta_1 x_1+\beta_2x_2+...+\beta_rx_r+e
\end{displaymath}
O, in caso di modello semplice:
\begin{displaymath}
    Y=\alpha+\beta x+e
\end{displaymath}
\subsection{Stima dei parametri di regressione}
Supponiamo di osservare, per $i$ che va da 1 a n, le risposte $Y_i$ corrispondenti a certi valori di ingresso $x_i$ e di volerle usare per stimare $\alpha$ e $\beta$ in un modello di regressione lineare semplice. Se A e B sono gli stimatori cercati, allora $A+Bx_i$ è lo stimatore della risposta. Poiché la risposta realmente ottenuta è $Y_i$, la quantità $(Y_i-A-Bx_i)^2$ rappresenta il quadrato della differenza tra predizione e valore osservato, e dovrebbe quindi essere minimo. Denotiamo con $SS$ la somma di questi dati:
\begin{displaymath}
    SS:=\sum_1^n (Y_i-A-Bx_i)^2
\end{displaymath}
Il metodo dei minimi quadrati consiste nello scegliere come stimatori di $\alpha$ e $\beta$ i due valori di $A$ e $B$ che minimizzano $SS$. Deriviamo SS rispetto ad A e B.
\begin{gather*}
    \frac{dSS}{dA} = -2 \sum_1^n (Y_i-A-Bx_i)\\ 
    \frac{dSS}{dB} = -2 \sum_1^n x_i(Y_i-A-Bx_i)
\end{gather*}
Per trovare il minimo, li poniamo a 0, ottenendo, dopo calcoli, gli stimatori dei minimi quadrati
\begin{displaymath}
    B=\frac{\sum_i x_iY_i-\overline{x}\sum_i Y_i}{\sum_i x_i^2-n\overline{x}^2}\hspace{20px}A=\overline{Y} - B\overline{x}
\end{displaymath}
La retta $A+Bx_i$ è la stima della retta di regressione, ossia la retta che interpola meglio i dati.
\subsection{Distribuzione dei dati}
Se fino ad ora è stato sufficiente supporre che gli errori casuali avessero media nulla, non possiamo illuderci per sempre. Scopriamo, riscrivendo $B$, che esso è una combinazione lineare delle normali $Y_1,...,Y_n$, ed è quindi normale. Calcolandone i parametri, otteniamo che $E[B] = \beta$, e quindi B è uno stimatore non distorto. Passando ad A, otteniamo che $E[A]=\alpha$ ed anch'esso è uno stimatore corretto. Calcoliamo la varianza di A esprimendolo come combinazione lineare di $Y_1,...,Y_n$. Il risultato è che
\begin{displaymath}
    Var(A)=\frac{\sigma^2\sum_i x_i^2}{n\left(\sum_i x_i^2-n\overline{x}^2\right)}
\end{displaymath}
Ritornando ai nostri residui $SS_R$, osserviamo che 
\begin{displaymath}
    \frac{SS_R}{\sigma^2} \sim \chi^2_{n-2}
\end{displaymath}
il fatto che sia una chi-quadro, implica che
\begin{displaymath}
    E\left[\frac{SS_R}{\sigma^2}\right] = n-2 \hspace{30px}E\left[\frac{SS_R}{n-2}\right] = \sigma^2
\end{displaymath}
Così che $\frac{SS_R}{n-2}$ è uno stimatore non distorto di $\sigma^2$. 
Definiamo ora una notazione che sarà comoda in futuro:
\begin{gather*}
    S_{xY} := \sum_1^n(x_i-\overline{x})(Y_i-\overline{Y})\\ 
    S_{xx} := \sum_1^n (x_i-\overline{x})^2\\ 
    S_{YY} := \sum_1^n (Y_i-\overline{Y})^2
\end{gather*}
Possiamo così riscrivere gli stimatori
\begin{displaymath}
    B = \frac{S_{xY}}{S_{xx}}\hspace{30px}A = \overline{Y}-B\overline{x}\hspace{30px}SS_R=\frac{S_{xx}S_{YY}-S^2_{xY}}{S_{xx}}
\end{displaymath}
\subsection{Inferenza statistica sui parametri di regressione}
Grazie al \textit{refactoring} appena fatto, costruire test statistici e intervalli di confidenza è \textbf{relativamente} semplice.
\subsubsection{Inferenza su $\beta$}
Un'ipotesi importante da verificare è quella che $\beta$ sia pari a zero. Di fatti, se è vera, la risposta non dipende dall'ingresso, ovvero non vi è correlazione tra le due variabili. Per verificare
\begin{displaymath}
    H_0: \beta=0 \mbox{ contro }H_1:\beta\neq0
\end{displaymath}
notiamo che 
\begin{displaymath}
    \frac{B-E[B]}{\sqrt{Var(B)}}=\frac{B-\beta}{\sigma/\sqrt{S_{xx}}} \sim \mathcal{N}(0,1)
\end{displaymath}
e inoltre tale V.A. è indipendente da
\begin{displaymath}
    \frac{SS_R}{\sigma^2} \sim \chi^2_{n-2}
\end{displaymath}
Perciò, dalla definizione di distribuzione $t$ segue che 
\begin{displaymath}
    \frac{B-\beta}{\sigma/\sqrt{S_{xx}}}\sqrt{\frac{\sigma^2(n-2)}{SS_R}}=\sqrt{\frac{(n-2)S_{xx}}{SS_R}}(B-\beta)\sim t_{n-2}
\end{displaymath}
Quindi, quando $H_0$ è vera,
\begin{displaymath}
    \sqrt{\frac{(n-2)S_{xx}}{SS_R}}B\sim t_{n-2}
\end{displaymath}
se volessimo avere un livello di significatività $\gamma$, quindi
\begin{gather*}
    \mbox{si rifiuta }H_0\mbox{ se }\sqrt{\frac{(n-2)S_{xx}}{SS_R}}|B|> t_{\frac{\gamma}{2},n-2}\\ 
    \mbox{si accetta }H_0\mbox{ negli altri casi}
\end{gather*}
Si potrebbe anche calcolare il valore $v$ assunto da $\sqrt{\frac{(n-2)S_{xx}}{SS_R}}|B|$ e rifiutando quindi $H_0$ quando il livello di significatività è maggiore o uguale a 
\begin{displaymath}
    \mbox{p-dei-dati}=P(|T_{n-2}|>v)=2P(T_{n-2}>v)
\end{displaymath}
\subsubsection{Regressione alla media}
Assumiamo che vi sia una relazione lineare tra il valore della stessa caratteristica in un figlio $(Y)$ ed un genitore $x$. Si avrà una regressione verso la media ogni volta che il parametro $\beta$ è compreso tra 0 e 1. Ovvero, se 
\begin{displaymath}
    E[Y]=\alpha+\beta x
\end{displaymath}
e $0<\beta<1$, allora $E[Y]$ sarà più piccolo di $x$ quando $x$ è molto grande, e più grande di $x$ quando $x$ è molto piccolo. 
\subsubsection{Inferenza su $\alpha$}
La determinazione degli intervalli di confidenza e dei test statistici che riguardano il parametro $\alpha$ si ottiene in modo analogo a quanto fatto per $\beta$. In particolare, possiamo mostrare che 
\begin{displaymath}
    \sqrt{\frac{n(n-2)S_{xx}}{SS_R\sum_i x_i^2}}(A-\alpha)\sim t_{n-2}
\end{displaymath}
di conseguenza, ad un livello $1-\gamma$, l'intervallo di confidenza bilaterale è dato da 
\begin{displaymath}
    A\pm t_{\frac{\gamma}{2},n-2}\sqrt{\frac{SS_R\sum_i x_i^2}{n(n-2)S_{xx}}}
\end{displaymath}
\subsubsection{Inferenza sulla risposta media $\alpha+\beta x$}
Una questione certamente interessante è l'utilizzo delle coppie di dati $(x_i, Y_i),i=1,2,...,n$ per stimare $\alpha+\beta x_0$, vale a dire la risposta media per un livello di ingresso assegnato $x_0$. Se si desidera uno stimatore puntuale, la scelta ovvia è $A+Bx_0$, che è uno stimatore non distorto, visto che A e B lo sono entrambi. Se invece vogliamo ottenere degli intervalli di confidenza, oppure verificare delle ipotesi sulla risposta media, è necessario prima determinare la distribuzione dello stimatore $A+Bx_0$, che scopriremo essere normale. La media già la conosciamo, ci serve la varianza
\begin{displaymath}
    Var(A+Bx_0)=\sigma^2\left[\frac{1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}\right]
\end{displaymath}
Quindi, in definitiva
\begin{displaymath}
    A+Bx_0 \sim \mathcal{N}\left(\alpha+\beta x_0, \sigma^2\left[\frac{1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}\right]\right)
\end{displaymath}
Non possiamo però fare inferenza su questa statistica perché $\sigma^2$ è incognita. Per questo, sfruttiamo il fatto che $A+Bx_0$ sia indipendente da
\begin{displaymath}
    \frac{SS_R}{\sigma^2}\sim \chi^2_{n-2}
\end{displaymath}
ricaviamo quindi gli intervalli di confidenza
\begin{displaymath}
    A+Bx_0 \pm t_{\frac{\gamma}{2},n-2}\sqrt{\frac{1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}}\sqrt{\frac{SS_R}{n-2}}
\end{displaymath}
\subsubsection{Intervallo di predizione di una risposta futura}
Ipotizziamo ora di voler trovare non un valore medio di $\alpha+\beta x$, quanto un singolo valore dato un ingresso specifico. Lo stimatore adatto sarebbe quello della sua media $A+Bx_0$. Se volessimo però un intervallo di valori che contenga la risposta con un determinato livello di confidenza?
Denotiamo semplicemente Y, la risposta futura con un livello di ingresso $x_0$, e consideriamo la distribuzione di probabilità di $Y-A-Bx_0$, cioè la differenza tra risposta e valore predetto. Sappiamo per ipotesi che Y è normale di parametri $\alpha+\beta x_0$ e $\sigma^2$. Sappiamo inoltre da prima che 
\begin{displaymath}
    A+Bx_0 \sim \mathcal{N}\left(\alpha+\beta x_0, \sigma^2\left[\frac{1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}\right]\right)
\end{displaymath}
Di conseguenza, la differenza è ancora una normale
\begin{displaymath}
    Y-A-Bx_0 \sim \mathcal{N}\left(0, \sigma^2\left[1+\frac{1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}\right]\right)
\end{displaymath}
Usando il fatto che $SS_R$ è indipendente da A e B, come pure da Y, e che $\frac{SS_R}{\sigma^2}$ è una chi-quadro con $n-2$ gradi di libertà, sostituiamo e otteniamo una $t$:
\begin{displaymath}
    \frac{Y-A-Bx_0}{\sqrt{\frac{n+1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}}\sqrt{\frac{SS_R}{n-2}}}\sim t_{n-2}
\end{displaymath} 
In definitiva, per ogni $0<\gamma<1$, si ha che
\begin{displaymath}
    P\left(-t_{\frac{\gamma}{2},n-2}  <      \frac{Y-A-Bx_0}{\sqrt{\frac{n+1}{n}+\frac{(\overline{x}-x_0)^2}{S_{xx}}}\sqrt{\frac{SS_R}{n-2}}}  <  t_{\frac{\gamma}{2},n-2}\right) = 1-\gamma
\end{displaymath}
\subsection{Coefficiente di determinazione e coefficiente di correlazione campionaria}
Supponiamo di voler esprimere la variabilità o dispersione dell'insieme di risposte $Y_1,...,Y_n$, ottenute con livelli di ingresso $x_1,...,x_n$. Una comune misura statistica della variabilità è costituita da 
\begin{displaymath}
    S_{YY} := \sum_1^n (Y_i-\overline{Y})^2
\end{displaymath}
una quantità che rappresenta, a meno di un fattore moltiplicativo, la varianza campionaria delle $Y_i$. Se esse fossero ad esempio tutte uguali tra loro -e quindi tutte uguali a $\overline{Y}$- il valore si $S_{YY}$ sarebbe pari a 0.
Questa variabilità viene però da due contributi. In primis, sappiamo che se le $x_i$ non sono tutte uguali, le $Y_i$ hanno valori attesi diversi, e questo disperde le loro realizzazioni. In secundis, una volta che si tenga conto della variabilità delle $x_i$, ogni $Y_i$ ha distribuzione con varianza $\sigma^2$ attorno al suo valore atteso e non coinciderà quindi esattamente con le nostre predizioni. Cerchiamo di quantificare quale parte della variabilità sia dovuta all'ingresso e quale alla varianza propria delle risposte. Notiamo che la quantità
\begin{displaymath}
    SS_R := \sum_1^n (Y_i-A-Bx_i)^2
\end{displaymath}
misura quella parte di variabilità intrinseca delle risposte. Di conseguenza, $S_{YY}- SS_R$ rappresenta l'altra parte, cioè quella che si spiega con la diversità dei livelli di ingresso. La statistica $R^2$, detta \textit{coefficiente di determinazione}, è la frazione della variabilità totale che si spiega con la diversità dei livelli di ingresso
\begin{displaymath}
    R^2 := \frac{S_{YY}-SS_R}{S_{YY}} = 1-\frac{SS_R}{S_{YY}}
\end{displaymath}
\subsection{Analisi dei residui: verifica del modello}
Per verificare se un modello di regressione lineare semplice
\begin{displaymath}
    Y=\alpha+\beta x + e
\end{displaymath}
si adatti o meno ai dati, analizziamo i cosiddetti \textit{residui}, $Y_i-(A+Bx_i)$. Per prima cosa essi vanno normalizzati, dividendoli per lo stimatore della deviazione standard delle $Y_i$:
\begin{displaymath}
    \frac{Y_i-(A+Bx_i)}{\sqrt{SS_R/(n-2)}}
\end{displaymath}
ottenenendo i cosiddetti \textit{residui standardizzati}. Quando il modello è corretto, questi residui sono approssimativamente normali standard indipendenti, quindi distribuiti attorno allo zero. 
\subsection{Linearizzazione}
In certe situazioni può essere evidente che la risposta non sia una funzione lineare del livello di ingresso. Possiamo però a volte riportarci al caso lineare con un cambiamento di variabili. Ad esempio, immaginiamo di avere una relazione esponenziale. Potremmo linearizzarla prendendo i logaritmi, e poi stimare $\alpha$ e $\beta$ con il metodo dei minimi quadrati. 
\subsection{Minimi quadrati pesati}
Nel solito modello di regressione $Y=\alpha+\beta x + e$ può capitare che la varianza delle risposte non sia costante ma dipenda dal livello di ingresso. Se queste dipendenze sono note, oppure se sono note a meno di un fattore moltiplicativo, i parametri di regressione si possono stimare minimizzando una somma pesata dei residui al quadrato. In particolare, se
\begin{displaymath}
    Var(Y_i) = \frac{\sigma^2}{w_i}
\end{displaymath}
con le $w_i$ note e $\sigma^2$ eventualmente ignota, allora gli stimatori A e B vanno scelti in modo da minimizzare 
\begin{displaymath}
    \sum_1^n \frac{[Y_i-(A+Bx_i)]^2}{Var(Y_i)} = \frac{1}{\sigma^2}\sum_1^n w_i(Y_i-A-Bx_i)^2
\end{displaymath}
Calcolando le derivate parziali rispetto ad A e B, otteniamo il seguente sistema
\begin{displaymath}
    \begin{cases}
        \sum_1^n w_iY_i = A\sum_1^n w_i + B \sum_1^n w_i x_i\\ 
        \sum_1^n w_i x_i Y_i = A\sum_1^n w_ix_i + B\sum_1^nw_ix_i^2
    \end{cases}
\end{displaymath}
Queste equazioni possono essere facilmente risolte per trovare gli stimatori dei minimi quadrati. 
\subsection{Regressione polinomiale}
Nei casi in cui la relazione che lega la variabile di risposta Y con quella indipendente $x$ non possa essere approssimata adeguatamente con modelli lineari, si può a volte prendere in considerazione le relazioni polinomiali, come
\begin{displaymath}
    U=\beta_0 + \beta_1 x + \beta_2x^2+...+\beta_rx^r+e
\end{displaymath}
dove i $\beta$ sono i coefficienti di regressione che è necessario stimare. Supponendo che i dati consistano di $n$ coppie di valori $(x_i,Y_i)$, gli stimatori dei minimi quadrati di $\beta_0,...,\beta_r$, che denotiamo con $B_1,...,B_r$, sono quei valori che rendono minima l'espressione seguente:
\begin{displaymath}
    \sum_1^n (Y_i-B_0-B_1x_i-B_rx_i^r)^2
\end{displaymath} 
Per determinarli calcoliamo le derivate parziali rispetto a $B_0,...,B_r$ e le poniamo uguali a 0. Otteniamo un sistema in $r+1$ equazioni dette \textit{equazioni normali}.
\begin{displaymath}
    \begin{cases}
        \sum_1^n Y_i = B_0n+B_1\sum_1^n x_i + B_2\sum_1^n x_i^2+...+B_r\sum_1^n x_i^r\\ 
        \sum_1^n x_iY_i = B_0\sum_1^n x_i+B_1\sum_1^n x_i^2 + B_2\sum_1^n x_i^3+...+B_r\sum_1^n x_i^{r+1}\\ 
        ...\\
        \sum_1^n x_i^rY_i = B_0\sum_1^n x_i^r+B_1\sum_1^n x_i^{r+1} + B_2\sum_1^n x_i^{r+2}+...+B_r\sum_1^n x_i^{2r} 
    \end{cases}
\end{displaymath}
La scelta del grado va svolta studiando il diagramma di dispersione, che spesso può darne un'idea.
\subsection{Regressione lineare multipla}
Nella gran parte delle applicazioni, la risposta può essere predetta in maniera migliore se, al posto di basarsi su una singola variabile, se ne utilizzano diverse. Studiamo il modello di regressione con $k$ variabili indipendenti, la risposta è legata loro tramite una relazione lineare
\begin{displaymath}
    Y=\beta_0+\beta_1 x_1 + ... + \beta_k x_k + e
\end{displaymath}
dove per $j$ che va da 1 a $k$, $x_j$ è il livello della j-esima variabile di ingresso ed $e$ è un errore casuale che assumeremo di media nulla e varianza costante incognita. I parametri $\beta$ devono essere stimati dai dati, che consisteranno di $n$ osservazioni di risposte $Y_i$ unitamente ai rispettivi livelli di ingresso. Le variabili $Y_i$ sono legate agli ingressi tramite
\begin{displaymath}
    E[Y_i] = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik}
\end{displaymath}
Se denotiamo con $B_0,...,B_k$ gli stimatori di $\beta$, allora la somma dei residui al quadrato è 
\begin{displaymath}
    \sum_1^n (Y_i-B_0- B_1x_{i1}-...-B_kx_{ik})^2
\end{displaymath}
ed è precisamente l'espressione da minimizzare dagli stimatori dei minimi quadrati. Per determinarli, facciamo le derivate parziali rispetto a $B_i$ e le poniamo uguali a 0, giungendo al sistema seguente:
\begin{displaymath}
    \begin{cases}
        \sum_1^n Y_i = B_0n+B_1\sum_1^n x_{i1} + B_2\sum_1^n x_{i2}+...+B_k\sum_1^n x_i^k\\ 
        \sum_1^n x_{i1}Y_i = B_0\sum_1^n x_{i1}+B_1\sum_1^n x_{i1}^2 + B_2\sum_1^n x_{i1}x_{i2}+...+B_k\sum_1^n x_{i1}x_{ik}\\ 
        ...\\
        \sum_1^n x_{ik}Y_i = B_0\sum_1^n x_{ik}+B_1\sum_1^n x_{i1}x_{ik} + B_2\sum_1^n x_{ik}x_{i2}+...+B_k\sum_1^n x_{ik}^2\\ 
    ...\\    \end{cases}
\end{displaymath}
Conviene introdurre una notazione matriciale.
\begin{displaymath}
    Y:=\left[\begin{matrix}
        Y_1\\
        Y_2\\ 
        ...\\
        Y_n
    \end{matrix}\right]
    \hspace{20px}
    X:=
    \left[
        \begin{matrix}
            1&x_{11}&x_{12}&...&x_{1k}\\ 
            1&x_{21}&x_{22}&...&x_{2k}\\
            ...&...&...&...&...\\ 
            1&x_{n1}&x_{n2}&...&x_{nk}
        \end{matrix}
    \right]
    \hspace{20px}
    \beta:=
    \left[
        \begin{matrix}
            \beta_0\\ 
            \beta_1\\ 
            ...\\ 
            \beta_k
        \end{matrix}
    \right]
    \hspace{20px}
    e:=
    \left[
        \begin{matrix}
            e_1\\ 
            e_2\\ 
            ...\\ 
            e_n
        \end{matrix}
    \right]
\end{displaymath}
Con questa notazione, il modello di regressione multipla può essere scritto nella forma 
\begin{displaymath}
    Y=X\beta + e
\end{displaymath}
Se inoltre denotiamo con 
\begin{displaymath}
    B:=
    \left[
        \begin{matrix}
            B_0\\ 
            B_1\\ 
            ...\\ 
            B_k            
        \end{matrix}
    \right]
\end{displaymath}
allora le equazioni normali prendono la forma 
\begin{displaymath}
    X'XB=X'Y
\end{displaymath}
dove $X'$ è la trasposta di $X$. Se poi $X'X$ è invertibile, possiamo ricavare gli stimatori dei minimi quadrati tramite
\begin{displaymath}
    B=(X'X)^{-1}X'Y
\end{displaymath}
\subsection{Formulario rapido della regressione}
\subsubsection{Regressione lineare}
\begin{gather*}
    S_{xY}=\sum_i (x_i-\overline{x})(Y_i-\overline{Y})\hspace{10px}S_{xx}=\sum_i(x_i-\overline{x})^2\hspace{10px}S_{YY}=\sum_i (Y_i-\overline{Y})^2\\ 
    \rho = \frac{S_{xY}}{\sqrt{S_{xx}S_{YY}}}\hspace{10px}B=\frac{S_{xY}}{S_{xx}}\hspace{10px}A=\overline{Y}-B\overline{x}\\ 
    R^2=1-\frac{SS_R}{S_{YY}}\hspace{10px}SS_R=\frac{S_{xx}S_{YY}-S{xY}^2}{S_{xx}}
\end{gather*}
\subsubsection{Regressione multipla}
\begin{gather*}
    Y=XB+e\hspace{10px}X'XB=X'Y \rightarrow B=(X'X)^{-1}X'Y\hspace{10px}C:=(X'X)^{-1}X'\rightarrow B=CY\\ 
    Cov(B)=\sigma C C' = \sigma (X'X)^{-1}\\ 
    \frac{SS_R}{\sigma^2}\sim \chi_{n-k+1}^2\rightarrow E\left[\frac{SS_R}{\sigma^2}\right]=n-k-1\\ 
    r=Y-XB\rightarrow SS_R=Y'Y-B'X'Y
\end{gather*}
\end{document}
